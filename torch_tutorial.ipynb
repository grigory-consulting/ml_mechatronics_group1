{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3551681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ceace385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1,2,3]) # tensor is n-dimensional array\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47a94ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4853, 0.1795, 0.6111],\n",
       "        [0.7453, 0.1173, 0.8784]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.rand(2,3) # random tensor, 2 rows, 3 columns\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41f62e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_tensor = torch.zeros(2,3)\n",
    "zeros_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7ce47e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_tensor = torch.ones(2,3)\n",
    "ones_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97453bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 4.],\n",
       "        [2., 3., 4.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + ones_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb90b80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 22.],\n",
       "        [43., 50.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.,2], [3,4]])\n",
    "b = torch.tensor([[5.,6], [7,8]])\n",
    "a @ b # torch.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5c9a7668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5., 12.],\n",
       "        [21., 32.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc8c5828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a1dc784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9310f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu, cuda, mps and possibly other frameworks \n",
    "\n",
    "# torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\") # for ordinary cpu\n",
    "device = torch.device(\"cpu\") # for CUDA\n",
    "device = torch.device(\"mps\") # for Apple Silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4ef07c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradients \n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True) # mode: propagate the gradients over all computation steps\n",
    "y = x**2 + 3*x +5\n",
    "# 2*x +3 -> 2*2 + 3 = 7\n",
    "y.backward() # compute gradients using backpropagation algorithm\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bccda",
   "metadata": {},
   "source": [
    "- easy NN, one gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47d58b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.092 (before update)\n",
      "Loss: 0.077 (after update)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn # Neural Networks \n",
    "import torch.optim as optim # optimizer framework for gradient methods \n",
    "\n",
    "# 10 inputs, 1 output -> fully connected feed-forward neural network\n",
    "NN = nn.Linear(10,1) # W*x + b, in Tensorflow -> Dense\n",
    "# MLP -> Multi-Layer Perceptron \n",
    "# in Literature/Publications: FC, FFN, FFNN, MLP \n",
    "\n",
    "loss = nn.MSELoss() # mean squared error\n",
    "# loss, cost, criterion, crit \n",
    "optimizer = optim.SGD(NN.parameters(), lr = 1e-2)\n",
    "# lr = learning rate, eta, alpha \n",
    "# NN.parameters() ... weights and biases\n",
    "\n",
    "input_data = torch.rand(10) # random stuff, X \n",
    "output = NN(input_data) # y_pred, predictions, y_hat \n",
    "y = torch.ones(1) # ground truth, target, regr \n",
    "\n",
    "# initial value of the loss function \n",
    "loss_output = loss(y, output) # Difference between reality and expectation \n",
    "print(f\"Loss: {loss_output:.2} (before update)\")\n",
    "\n",
    "# two magical lines\n",
    "loss_output.backward() # compute gradients\n",
    "optimizer.step() # Update the weights and biases \n",
    "\n",
    "output_new = NN(input_data) # here are new weights and biases \n",
    "loss_new = loss(output_new,y) # value of the loss function after the update\n",
    "print(f\"Loss: {loss_new:.2} (after update)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a281870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61a7dd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6965], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output # before Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8aacfb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7227], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfdbfd",
   "metadata": {},
   "source": [
    "# FashionMNIST dataset \n",
    "- modern HelloWorld for NNs (by Zalando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f18bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert to tensor\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33cc8d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f76b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "\n",
    "class NN(nn.Module): # class \n",
    "\n",
    "    def __init__(self):\n",
    "        super(NN,self).__init__() # init from the superclass\n",
    "        self.layer1 = nn.Linear(28*28, 128 ) # input layer \n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.layer2 = nn.Linear(128, 64) # hidden layer\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.layer3 = nn.Linear(64 ,10)         # output layer, 10 number of product categories\n",
    "        self.drop = nn.Dropout(0.3) # 30 % of activations are set to zero\n",
    "        # 0.1 -> 0.2 -> ... -> 0.5 \n",
    "    \n",
    "    # First way to apply BatchNorm (preactivation)\n",
    "    def forward(self,x): # propagate the information through the network\n",
    "        x = x.view(-1, 28*28) # flatten 2D -> 1D \n",
    "        x = torch.relu(self.bn1(self.layer1(x))) # activation using ReLU\n",
    "        x = self.drop(x) \n",
    "        x = torch.relu(self.bn2(self.layer2(x))) \n",
    "        x = self.drop(x) # here dropout is usually placed \n",
    "        x = self.layer3(x) # identity activation -> logit (no need to compute gradients of softmax)\n",
    "        return x \n",
    "    # Second way to apply BatchNorm (postactivation)\n",
    "    def forward(self,x): # propagate the information through the network\n",
    "        x = x.view(-1, 28*28) # flatten 2D -> 1D \n",
    "        x = torch.relu(self.layer1(x)) # activation using ReLU\n",
    "        x = self.bn1(x)\n",
    "        x = self.drop(x) \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.drop(x) # here dropout is usually placed \n",
    "        x = self.layer3(x) # identity activation -> logit (no need to compute gradients of softmax)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e5223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/40], Loss: 556.85595703125\n",
      "Epoch[2/40], Loss: 448.8724365234375\n",
      "Epoch[3/40], Loss: 418.54931640625\n",
      "Epoch[4/40], Loss: 404.9556579589844\n",
      "Epoch[5/40], Loss: 393.7926025390625\n",
      "Epoch[6/40], Loss: 378.41943359375\n",
      "Epoch[7/40], Loss: 372.0762634277344\n",
      "Epoch[8/40], Loss: 368.5741271972656\n",
      "Epoch[9/40], Loss: 370.3645935058594\n",
      "Epoch[10/40], Loss: 365.1058044433594\n",
      "Epoch[11/40], Loss: 355.17913818359375\n",
      "Epoch[12/40], Loss: 358.1354064941406\n",
      "Epoch[13/40], Loss: 351.5055236816406\n",
      "Epoch[14/40], Loss: 351.2488708496094\n",
      "Epoch[15/40], Loss: 347.54376220703125\n",
      "Epoch[16/40], Loss: 343.14398193359375\n",
      "Epoch[17/40], Loss: 339.050537109375\n",
      "Epoch[18/40], Loss: 337.1782531738281\n",
      "Epoch[19/40], Loss: 335.6484069824219\n",
      "Epoch[20/40], Loss: 331.34918212890625\n",
      "Epoch[21/40], Loss: 329.5379943847656\n",
      "Epoch[22/40], Loss: 323.7035217285156\n",
      "Epoch[23/40], Loss: 324.279296875\n",
      "Epoch[24/40], Loss: 320.00604248046875\n",
      "Epoch[25/40], Loss: 319.33050537109375\n",
      "Epoch[26/40], Loss: 318.7980041503906\n",
      "Epoch[27/40], Loss: 320.01556396484375\n",
      "Epoch[28/40], Loss: 315.6280822753906\n",
      "Epoch[29/40], Loss: 313.95281982421875\n",
      "Epoch[30/40], Loss: 312.9656982421875\n",
      "Epoch[31/40], Loss: 312.9793395996094\n",
      "Epoch[32/40], Loss: 310.4544982910156\n",
      "Epoch[33/40], Loss: 309.0552673339844\n",
      "Epoch[34/40], Loss: 311.2867126464844\n",
      "Epoch[35/40], Loss: 306.51116943359375\n",
      "Epoch[36/40], Loss: 307.9549560546875\n",
      "Epoch[37/40], Loss: 304.9294128417969\n",
      "Epoch[38/40], Loss: 300.6664733886719\n",
      "Epoch[39/40], Loss: 302.2656555175781\n",
      "Epoch[40/40], Loss: 303.7134704589844\n"
     ]
    }
   ],
   "source": [
    "# Now training\n",
    "\n",
    "model = NN()\n",
    "lr = 1e-2 # learning rate\n",
    "loss = nn.CrossEntropyLoss() # CE because multi-class problem\n",
    "optimizer = optim.SGD(model.parameters(), lr = lr )\n",
    "\n",
    "n_epochs = 20 \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # train mode\n",
    "    running_loss = 0.0 # loss per epoch\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad() # reset the gradients\n",
    "        # forward \n",
    "        outputs = model(images) # calculate outputs\n",
    "        curr_loss = loss(outputs,labels)\n",
    "        running_loss += curr_loss\n",
    "        # backward\n",
    "        curr_loss.backward() # gradients\n",
    "        optimizer.step()     # update weights and biases\n",
    "    print(f\"Epoch[{epoch + 1}/{n_epochs}], Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0364fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "97f5cc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d85d1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.67%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation \n",
    "\n",
    "model.eval() # setting the model to evaluation mode (implementation optimization)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): # we are not interested in gradient anymore \n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        predicted = torch.max(outputs.data, 1)[-1] \n",
    "        total += labels.size(0) \n",
    "        correct += (predicted==labels).sum().item()\n",
    "\n",
    "accuracy = correct/total\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
