{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f4f5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.57.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl (994 kB)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6daf1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd) # create Q, K, V \n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd) \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self,x):\n",
    "        B, T, C = x.size() # B .. batch_size, T... sequence length, C = n_embd \n",
    "        qkv = self.c_attn(x)\n",
    "        q,k,v = qkv.split(self.n_embd, dim = 2) # split \n",
    "        # n_head = nh, C = nh*hs, \n",
    "        # C = 768, nh = 12, hs = 64 \n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q,k,v, is_causal=True) # causal = masked\n",
    "        # Attention(q,k,v) = softmax(q*k/sqrt(hs))*v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # concatenate the results of head in to one vector \n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y \n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd) \n",
    "        self.gelu = nn.GELU() # Gaussian Error Linear Unit\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config) # Causal Self-Attention = Masked Self-Attention \n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config) # MLP = Positionwise FFN \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x)) # Prenorm\n",
    "        x = x + self.mlp(self.ln_2(x)) \n",
    "        return x\n",
    "\n",
    "@dataclass # only for configuration\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # maximum number of positions = context window \n",
    "    vocab_size: int = 50257 # number of tokens (gpt2-tokenizer)\n",
    "    n_embd: int = 768 # embedding dimension (tokens -> vectors)\n",
    "    n_layer: int = 12 # number of transformer layers\n",
    "    n_head: int = 12 # number of parallel heads in attention layer\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # token embedding (tokens to vectors)\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # position embedding \n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # (vectors to tokens) W*x \n",
    "    \n",
    "    def forward(self,x, targets = None): # split between training mode and inference mode \n",
    "        # x = (B,T) ... B is batch_size, T is sequence length \n",
    "        B, T = x.size()\n",
    "        assert T <= self.config.block_size \n",
    "        pos = torch.arange(0, T, dtype = torch.long, device = x.device) # 0,1,2,3, .... , T \n",
    "        tok_emb = self.transformer.wte(x)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = tok_emb + pos_emb \n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # FC \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0260345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('goethe.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85154b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 66404 tokens\n",
      "1 epoch = 129 batches\n",
      "step 0, loss: 11.0084228515625, dt: 58.76 ms, tokens/sec: 8713.70\n",
      "step 1, loss: 8.907684326171875, dt: 55.45 ms, tokens/sec: 9233.75\n",
      "step 2, loss: 8.362091064453125, dt: 56.00 ms, tokens/sec: 9142.97\n",
      "step 3, loss: 8.219306945800781, dt: 56.16 ms, tokens/sec: 9117.12\n",
      "step 4, loss: 7.812751770019531, dt: 55.46 ms, tokens/sec: 9231.76\n",
      "step 5, loss: 7.248233795166016, dt: 55.20 ms, tokens/sec: 9275.35\n",
      "step 6, loss: 7.34608268737793, dt: 56.26 ms, tokens/sec: 9100.97\n",
      "step 7, loss: 7.650753974914551, dt: 56.25 ms, tokens/sec: 9101.59\n",
      "step 8, loss: 6.7769575119018555, dt: 56.08 ms, tokens/sec: 9129.84\n",
      "step 9, loss: 6.876767158508301, dt: 56.02 ms, tokens/sec: 9140.06\n",
      "step 10, loss: 6.931089401245117, dt: 56.31 ms, tokens/sec: 9092.84\n",
      "step 11, loss: 6.996197700500488, dt: 55.04 ms, tokens/sec: 9302.14\n",
      "step 12, loss: 6.708436965942383, dt: 56.05 ms, tokens/sec: 9134.03\n",
      "step 13, loss: 6.1673736572265625, dt: 56.21 ms, tokens/sec: 9109.31\n",
      "step 14, loss: 6.242340087890625, dt: 56.36 ms, tokens/sec: 9084.92\n",
      "step 15, loss: 6.238173484802246, dt: 56.72 ms, tokens/sec: 9026.30\n",
      "step 16, loss: 6.248390197753906, dt: 56.22 ms, tokens/sec: 9107.03\n",
      "step 17, loss: 6.133028030395508, dt: 56.06 ms, tokens/sec: 9133.37\n",
      "step 18, loss: 6.200845718383789, dt: 55.07 ms, tokens/sec: 9297.71\n",
      "step 19, loss: 5.928240776062012, dt: 56.41 ms, tokens/sec: 9076.78\n",
      "step 20, loss: 6.008401870727539, dt: 56.34 ms, tokens/sec: 9087.19\n",
      "step 21, loss: 5.626945495605469, dt: 56.65 ms, tokens/sec: 9038.12\n",
      "step 22, loss: 5.664884567260742, dt: 56.36 ms, tokens/sec: 9084.80\n",
      "step 23, loss: 5.84863805770874, dt: 55.51 ms, tokens/sec: 9223.71\n",
      "step 24, loss: 5.663078308105469, dt: 56.31 ms, tokens/sec: 9092.53\n",
      "step 25, loss: 5.660333633422852, dt: 55.69 ms, tokens/sec: 9194.25\n",
      "step 26, loss: 5.736456871032715, dt: 55.81 ms, tokens/sec: 9173.48\n",
      "step 27, loss: 5.778477668762207, dt: 55.08 ms, tokens/sec: 9296.39\n",
      "step 28, loss: 5.679670333862305, dt: 56.49 ms, tokens/sec: 9063.41\n",
      "step 29, loss: 5.466248512268066, dt: 56.60 ms, tokens/sec: 9046.57\n",
      "step 30, loss: 5.587642669677734, dt: 56.29 ms, tokens/sec: 9096.08\n",
      "step 31, loss: 4.985282897949219, dt: 55.17 ms, tokens/sec: 9280.92\n",
      "step 32, loss: 5.288082122802734, dt: 56.66 ms, tokens/sec: 9035.87\n",
      "step 33, loss: 5.593760013580322, dt: 56.25 ms, tokens/sec: 9101.74\n",
      "step 34, loss: 5.164012908935547, dt: 56.80 ms, tokens/sec: 9014.06\n",
      "step 35, loss: 5.118864059448242, dt: 56.44 ms, tokens/sec: 9071.06\n",
      "step 36, loss: 5.018919944763184, dt: 56.62 ms, tokens/sec: 9042.91\n",
      "step 37, loss: 5.270798683166504, dt: 56.74 ms, tokens/sec: 9023.91\n",
      "step 38, loss: 5.360321998596191, dt: 58.96 ms, tokens/sec: 8683.44\n",
      "step 39, loss: 5.91657829284668, dt: 57.09 ms, tokens/sec: 8968.44\n",
      "step 40, loss: 4.694321632385254, dt: 56.12 ms, tokens/sec: 9123.13\n",
      "step 41, loss: 5.083732604980469, dt: 57.05 ms, tokens/sec: 8974.70\n",
      "step 42, loss: 4.911651611328125, dt: 56.73 ms, tokens/sec: 9024.71\n",
      "step 43, loss: 5.255682945251465, dt: 56.51 ms, tokens/sec: 9060.50\n",
      "step 44, loss: 4.907192230224609, dt: 56.24 ms, tokens/sec: 9103.67\n",
      "step 45, loss: 5.051721572875977, dt: 56.46 ms, tokens/sec: 9067.88\n",
      "step 46, loss: 5.138227462768555, dt: 56.11 ms, tokens/sec: 9125.26\n",
      "step 47, loss: 4.862569808959961, dt: 56.44 ms, tokens/sec: 9071.41\n",
      "step 48, loss: 5.1582536697387695, dt: 55.85 ms, tokens/sec: 9166.78\n",
      "step 49, loss: 5.02960205078125, dt: 56.80 ms, tokens/sec: 9014.86\n",
      "step 50, loss: 5.223264217376709, dt: 56.36 ms, tokens/sec: 9083.80\n",
      "step 51, loss: 4.777922630310059, dt: 56.88 ms, tokens/sec: 9001.26\n",
      "step 52, loss: 5.846991539001465, dt: 55.36 ms, tokens/sec: 9248.86\n",
      "step 53, loss: 5.71983528137207, dt: 55.38 ms, tokens/sec: 9245.24\n",
      "step 54, loss: 5.534021854400635, dt: 56.63 ms, tokens/sec: 9041.16\n",
      "step 55, loss: 5.45905876159668, dt: 56.52 ms, tokens/sec: 9059.39\n",
      "step 56, loss: 4.72122859954834, dt: 55.74 ms, tokens/sec: 9186.03\n",
      "step 57, loss: 4.944566249847412, dt: 55.73 ms, tokens/sec: 9186.98\n",
      "step 58, loss: 5.109614372253418, dt: 56.70 ms, tokens/sec: 9029.95\n",
      "step 59, loss: 5.043143272399902, dt: 56.46 ms, tokens/sec: 9068.88\n",
      "step 60, loss: 5.246310234069824, dt: 55.43 ms, tokens/sec: 9236.37\n",
      "step 61, loss: 5.4974589347839355, dt: 55.46 ms, tokens/sec: 9231.05\n",
      "step 62, loss: 4.784778594970703, dt: 56.92 ms, tokens/sec: 8995.22\n",
      "step 63, loss: 4.4414496421813965, dt: 55.39 ms, tokens/sec: 9243.41\n",
      "step 64, loss: 5.358461380004883, dt: 55.38 ms, tokens/sec: 9244.72\n",
      "step 65, loss: 5.165897846221924, dt: 56.14 ms, tokens/sec: 9119.37\n",
      "step 66, loss: 4.8379316329956055, dt: 57.38 ms, tokens/sec: 8922.35\n",
      "step 67, loss: 4.972887992858887, dt: 55.74 ms, tokens/sec: 9185.48\n",
      "step 68, loss: 4.899162292480469, dt: 55.21 ms, tokens/sec: 9273.66\n",
      "step 69, loss: 4.951786518096924, dt: 56.40 ms, tokens/sec: 9077.70\n",
      "step 70, loss: 4.477109909057617, dt: 56.83 ms, tokens/sec: 9009.00\n",
      "step 71, loss: 4.463718414306641, dt: 56.67 ms, tokens/sec: 9034.12\n",
      "step 72, loss: 5.325309753417969, dt: 56.78 ms, tokens/sec: 9017.09\n",
      "step 73, loss: 4.743537425994873, dt: 56.80 ms, tokens/sec: 9014.14\n",
      "step 74, loss: 5.320340156555176, dt: 56.84 ms, tokens/sec: 9008.09\n",
      "step 75, loss: 4.781269073486328, dt: 56.46 ms, tokens/sec: 9069.15\n",
      "step 76, loss: 4.927332401275635, dt: 57.33 ms, tokens/sec: 8930.14\n",
      "step 77, loss: 5.264028549194336, dt: 56.75 ms, tokens/sec: 9021.41\n",
      "step 78, loss: 4.720720291137695, dt: 56.46 ms, tokens/sec: 9069.00\n",
      "step 79, loss: 4.549643516540527, dt: 55.55 ms, tokens/sec: 9217.26\n",
      "step 80, loss: 5.168887615203857, dt: 56.27 ms, tokens/sec: 9098.62\n",
      "step 81, loss: 4.660915374755859, dt: 55.25 ms, tokens/sec: 9267.62\n",
      "step 82, loss: 4.300678253173828, dt: 56.78 ms, tokens/sec: 9017.40\n",
      "step 83, loss: 4.6020379066467285, dt: 56.14 ms, tokens/sec: 9119.87\n",
      "step 84, loss: 4.777400970458984, dt: 56.46 ms, tokens/sec: 9068.42\n",
      "step 85, loss: 4.886177062988281, dt: 56.32 ms, tokens/sec: 9090.30\n",
      "step 86, loss: 4.677794456481934, dt: 56.14 ms, tokens/sec: 9120.38\n",
      "step 87, loss: 5.133829116821289, dt: 56.52 ms, tokens/sec: 9058.93\n",
      "step 88, loss: 4.309192180633545, dt: 56.74 ms, tokens/sec: 9023.15\n",
      "step 89, loss: 5.343226432800293, dt: 56.10 ms, tokens/sec: 9126.54\n",
      "step 90, loss: 4.522622108459473, dt: 55.48 ms, tokens/sec: 9228.55\n",
      "step 91, loss: 4.756345748901367, dt: 56.79 ms, tokens/sec: 9016.34\n",
      "step 92, loss: 4.7080888748168945, dt: 56.76 ms, tokens/sec: 9019.93\n",
      "step 93, loss: 4.234247207641602, dt: 56.48 ms, tokens/sec: 9064.98\n",
      "step 94, loss: 4.477679252624512, dt: 55.72 ms, tokens/sec: 9188.31\n",
      "step 95, loss: 4.305042743682861, dt: 56.65 ms, tokens/sec: 9037.89\n",
      "step 96, loss: 4.255038261413574, dt: 56.14 ms, tokens/sec: 9120.38\n",
      "step 97, loss: 4.6090240478515625, dt: 57.29 ms, tokens/sec: 8936.34\n",
      "step 98, loss: 5.197688102722168, dt: 56.17 ms, tokens/sec: 9114.84\n",
      "step 99, loss: 5.0601911544799805, dt: 56.18 ms, tokens/sec: 9112.79\n",
      "step 100, loss: 5.168493270874023, dt: 56.46 ms, tokens/sec: 9067.58\n",
      "step 101, loss: 4.290403366088867, dt: 55.94 ms, tokens/sec: 9153.15\n",
      "step 102, loss: 4.451998233795166, dt: 56.10 ms, tokens/sec: 9126.85\n",
      "step 103, loss: 4.665355682373047, dt: 56.59 ms, tokens/sec: 9048.21\n",
      "step 104, loss: 5.236816883087158, dt: 55.24 ms, tokens/sec: 9267.98\n",
      "step 105, loss: 6.051746368408203, dt: 56.09 ms, tokens/sec: 9128.83\n",
      "step 106, loss: 5.157779693603516, dt: 56.36 ms, tokens/sec: 9083.84\n",
      "step 107, loss: 5.533875942230225, dt: 56.13 ms, tokens/sec: 9122.00\n",
      "step 108, loss: 5.129246711730957, dt: 55.17 ms, tokens/sec: 9280.24\n",
      "step 109, loss: 5.204324722290039, dt: 56.44 ms, tokens/sec: 9072.06\n",
      "step 110, loss: 5.149918556213379, dt: 55.58 ms, tokens/sec: 9211.96\n",
      "step 111, loss: 4.705062389373779, dt: 57.02 ms, tokens/sec: 8979.13\n",
      "step 112, loss: 4.759160041809082, dt: 56.64 ms, tokens/sec: 9040.32\n",
      "step 113, loss: 4.678297996520996, dt: 56.26 ms, tokens/sec: 9101.24\n",
      "step 114, loss: 4.781770706176758, dt: 55.32 ms, tokens/sec: 9255.92\n",
      "step 115, loss: 4.54879903793335, dt: 55.56 ms, tokens/sec: 9215.28\n",
      "step 116, loss: 4.532102584838867, dt: 55.94 ms, tokens/sec: 9152.64\n",
      "step 117, loss: 5.584331035614014, dt: 56.21 ms, tokens/sec: 9109.31\n",
      "step 118, loss: 5.317955493927002, dt: 55.29 ms, tokens/sec: 9259.63\n",
      "step 119, loss: 5.530635356903076, dt: 56.44 ms, tokens/sec: 9071.91\n",
      "step 120, loss: 5.366902828216553, dt: 56.22 ms, tokens/sec: 9106.92\n",
      "step 121, loss: 5.57347297668457, dt: 56.28 ms, tokens/sec: 9097.23\n",
      "step 122, loss: 6.18585205078125, dt: 56.01 ms, tokens/sec: 9141.34\n",
      "step 123, loss: 4.755253314971924, dt: 56.00 ms, tokens/sec: 9143.48\n",
      "step 124, loss: 4.799907207489014, dt: 56.10 ms, tokens/sec: 9126.89\n",
      "step 125, loss: 4.466292381286621, dt: 56.11 ms, tokens/sec: 9124.60\n",
      "step 126, loss: 4.311256408691406, dt: 56.14 ms, tokens/sec: 9119.37\n",
      "step 127, loss: 4.266792297363281, dt: 56.78 ms, tokens/sec: 9017.55\n",
      "step 128, loss: 4.2799973487854, dt: 56.26 ms, tokens/sec: 9101.40\n",
      "step 129, loss: 5.91083288192749, dt: 56.24 ms, tokens/sec: 9103.98\n",
      "step 130, loss: 5.0727219581604, dt: 55.51 ms, tokens/sec: 9224.23\n",
      "step 131, loss: 5.0810136795043945, dt: 56.28 ms, tokens/sec: 9096.73\n",
      "step 132, loss: 5.108633041381836, dt: 56.60 ms, tokens/sec: 9045.27\n",
      "step 133, loss: 4.899001121520996, dt: 56.89 ms, tokens/sec: 8999.82\n",
      "step 134, loss: 4.797623634338379, dt: 56.51 ms, tokens/sec: 9059.55\n",
      "step 135, loss: 4.996579647064209, dt: 55.44 ms, tokens/sec: 9235.89\n",
      "step 136, loss: 4.766467571258545, dt: 56.46 ms, tokens/sec: 9068.88\n",
      "step 137, loss: 4.2864227294921875, dt: 56.25 ms, tokens/sec: 9102.86\n",
      "step 138, loss: 4.73117733001709, dt: 55.66 ms, tokens/sec: 9198.43\n",
      "step 139, loss: 5.218100547790527, dt: 55.77 ms, tokens/sec: 9181.05\n",
      "step 140, loss: 5.257772445678711, dt: 57.37 ms, tokens/sec: 8924.16\n",
      "step 141, loss: 4.929837703704834, dt: 56.61 ms, tokens/sec: 9044.17\n",
      "step 142, loss: 4.540948867797852, dt: 55.69 ms, tokens/sec: 9193.07\n",
      "step 143, loss: 4.731558322906494, dt: 56.53 ms, tokens/sec: 9056.49\n",
      "step 144, loss: 4.870410919189453, dt: 56.52 ms, tokens/sec: 9058.86\n",
      "step 145, loss: 5.06013298034668, dt: 56.41 ms, tokens/sec: 9076.93\n",
      "step 146, loss: 4.955593585968018, dt: 56.00 ms, tokens/sec: 9142.24\n",
      "step 147, loss: 5.123629570007324, dt: 56.59 ms, tokens/sec: 9048.02\n",
      "step 148, loss: 4.9327802658081055, dt: 55.46 ms, tokens/sec: 9232.04\n",
      "step 149, loss: 4.912504196166992, dt: 57.10 ms, tokens/sec: 8967.20\n",
      "step 150, loss: 4.608646392822266, dt: 57.91 ms, tokens/sec: 8841.93\n",
      "step 151, loss: 4.670371055603027, dt: 56.74 ms, tokens/sec: 9023.65\n",
      "step 152, loss: 4.863672733306885, dt: 56.05 ms, tokens/sec: 9134.38\n",
      "step 153, loss: 4.763175010681152, dt: 57.56 ms, tokens/sec: 8894.74\n",
      "step 154, loss: 4.727808952331543, dt: 56.31 ms, tokens/sec: 9092.03\n",
      "step 155, loss: 4.839632987976074, dt: 57.77 ms, tokens/sec: 8862.00\n",
      "step 156, loss: 4.905945301055908, dt: 57.08 ms, tokens/sec: 8970.35\n",
      "step 157, loss: 4.649656295776367, dt: 57.04 ms, tokens/sec: 8975.67\n",
      "step 158, loss: 4.435640335083008, dt: 56.23 ms, tokens/sec: 9104.99\n",
      "step 159, loss: 4.718967914581299, dt: 55.09 ms, tokens/sec: 9294.17\n",
      "step 160, loss: 4.201111316680908, dt: 56.52 ms, tokens/sec: 9059.20\n",
      "step 161, loss: 4.265438079833984, dt: 56.23 ms, tokens/sec: 9106.26\n",
      "step 162, loss: 4.410218238830566, dt: 55.15 ms, tokens/sec: 9283.08\n",
      "step 163, loss: 4.0964202880859375, dt: 56.28 ms, tokens/sec: 9096.92\n",
      "step 164, loss: 3.737865924835205, dt: 55.17 ms, tokens/sec: 9280.92\n",
      "step 165, loss: 4.21042537689209, dt: 56.31 ms, tokens/sec: 9092.84\n",
      "step 166, loss: 4.268825054168701, dt: 56.52 ms, tokens/sec: 9057.94\n",
      "step 167, loss: 4.409137725830078, dt: 55.26 ms, tokens/sec: 9266.10\n",
      "step 168, loss: 4.756774425506592, dt: 56.25 ms, tokens/sec: 9102.98\n",
      "step 169, loss: 3.8400115966796875, dt: 55.16 ms, tokens/sec: 9281.44\n",
      "step 170, loss: 4.327361106872559, dt: 56.31 ms, tokens/sec: 9092.69\n",
      "step 171, loss: 4.009070873260498, dt: 56.17 ms, tokens/sec: 9114.57\n",
      "step 172, loss: 4.414322376251221, dt: 55.62 ms, tokens/sec: 9204.50\n",
      "step 173, loss: 4.114812850952148, dt: 56.25 ms, tokens/sec: 9102.71\n",
      "step 174, loss: 4.241922378540039, dt: 56.25 ms, tokens/sec: 9102.90\n",
      "step 175, loss: 4.228452682495117, dt: 56.23 ms, tokens/sec: 9104.68\n",
      "step 176, loss: 4.011794090270996, dt: 56.13 ms, tokens/sec: 9120.88\n",
      "step 177, loss: 4.384125232696533, dt: 56.17 ms, tokens/sec: 9114.41\n",
      "step 178, loss: 4.183577537536621, dt: 56.23 ms, tokens/sec: 9106.07\n",
      "step 179, loss: 4.434589385986328, dt: 56.22 ms, tokens/sec: 9106.57\n",
      "step 180, loss: 3.8774213790893555, dt: 56.16 ms, tokens/sec: 9116.66\n",
      "step 181, loss: 4.852574348449707, dt: 56.24 ms, tokens/sec: 9103.67\n",
      "step 182, loss: 4.689593315124512, dt: 56.61 ms, tokens/sec: 9044.47\n",
      "step 183, loss: 4.573031425476074, dt: 56.41 ms, tokens/sec: 9077.05\n",
      "step 184, loss: 4.312384128570557, dt: 56.67 ms, tokens/sec: 9034.12\n",
      "step 185, loss: 3.81138277053833, dt: 56.00 ms, tokens/sec: 9142.35\n",
      "step 186, loss: 4.001204967498779, dt: 56.46 ms, tokens/sec: 9068.69\n",
      "step 187, loss: 4.067129135131836, dt: 56.52 ms, tokens/sec: 9057.94\n",
      "step 188, loss: 4.100544452667236, dt: 56.45 ms, tokens/sec: 9069.95\n",
      "step 189, loss: 4.096951007843018, dt: 56.46 ms, tokens/sec: 9068.08\n",
      "step 190, loss: 4.514388561248779, dt: 56.36 ms, tokens/sec: 9085.07\n",
      "step 191, loss: 4.0724263191223145, dt: 56.33 ms, tokens/sec: 9089.61\n",
      "step 192, loss: 3.7002243995666504, dt: 56.73 ms, tokens/sec: 9025.85\n",
      "step 193, loss: 4.494688987731934, dt: 57.39 ms, tokens/sec: 8921.12\n",
      "step 194, loss: 4.2775163650512695, dt: 56.41 ms, tokens/sec: 9077.20\n",
      "step 195, loss: 4.0150465965271, dt: 55.32 ms, tokens/sec: 9256.08\n",
      "step 196, loss: 4.153626441955566, dt: 56.49 ms, tokens/sec: 9062.76\n",
      "step 197, loss: 4.208959579467773, dt: 56.19 ms, tokens/sec: 9111.94\n",
      "step 198, loss: 4.192956447601318, dt: 56.17 ms, tokens/sec: 9114.88\n",
      "step 199, loss: 3.8972558975219727, dt: 57.45 ms, tokens/sec: 8912.27\n",
      "step 200, loss: 3.7968130111694336, dt: 56.33 ms, tokens/sec: 9089.45\n",
      "step 201, loss: 4.659760475158691, dt: 56.29 ms, tokens/sec: 9095.73\n",
      "step 202, loss: 4.026871681213379, dt: 56.89 ms, tokens/sec: 8999.63\n",
      "step 203, loss: 4.588468551635742, dt: 55.28 ms, tokens/sec: 9262.26\n",
      "step 204, loss: 4.090603828430176, dt: 55.48 ms, tokens/sec: 9229.07\n",
      "step 205, loss: 4.250986099243164, dt: 56.34 ms, tokens/sec: 9087.53\n",
      "step 206, loss: 4.223509311676025, dt: 56.52 ms, tokens/sec: 9058.59\n",
      "step 207, loss: 3.727226972579956, dt: 56.32 ms, tokens/sec: 9091.19\n",
      "step 208, loss: 3.670781135559082, dt: 56.26 ms, tokens/sec: 9099.82\n",
      "step 209, loss: 4.333404541015625, dt: 56.39 ms, tokens/sec: 9079.42\n",
      "step 210, loss: 3.865293502807617, dt: 56.86 ms, tokens/sec: 9003.78\n",
      "step 211, loss: 3.622323751449585, dt: 57.24 ms, tokens/sec: 8944.20\n",
      "step 212, loss: 4.067939758300781, dt: 55.44 ms, tokens/sec: 9235.06\n",
      "step 213, loss: 3.883263111114502, dt: 56.56 ms, tokens/sec: 9052.37\n",
      "step 214, loss: 4.187094688415527, dt: 56.39 ms, tokens/sec: 9078.96\n",
      "step 215, loss: 3.91808819770813, dt: 56.24 ms, tokens/sec: 9103.21\n",
      "step 216, loss: 4.211833953857422, dt: 57.05 ms, tokens/sec: 8974.29\n",
      "step 217, loss: 3.470296859741211, dt: 57.02 ms, tokens/sec: 8978.86\n",
      "step 218, loss: 4.621099948883057, dt: 56.71 ms, tokens/sec: 9028.84\n",
      "step 219, loss: 4.030688285827637, dt: 56.45 ms, tokens/sec: 9070.76\n",
      "step 220, loss: 4.193818092346191, dt: 56.95 ms, tokens/sec: 8990.63\n",
      "step 221, loss: 4.165584087371826, dt: 56.42 ms, tokens/sec: 9074.17\n",
      "step 222, loss: 3.6860790252685547, dt: 56.48 ms, tokens/sec: 9065.74\n",
      "step 223, loss: 3.8070731163024902, dt: 55.39 ms, tokens/sec: 9243.88\n",
      "step 224, loss: 3.7670974731445312, dt: 56.20 ms, tokens/sec: 9110.86\n",
      "step 225, loss: 3.6969385147094727, dt: 55.17 ms, tokens/sec: 9280.56\n",
      "step 226, loss: 3.9707648754119873, dt: 57.79 ms, tokens/sec: 8859.04\n",
      "step 227, loss: 4.4846954345703125, dt: 56.59 ms, tokens/sec: 9047.52\n",
      "step 228, loss: 4.372296333312988, dt: 56.18 ms, tokens/sec: 9114.34\n",
      "step 229, loss: 4.490603446960449, dt: 56.49 ms, tokens/sec: 9062.91\n",
      "step 230, loss: 3.786020278930664, dt: 57.14 ms, tokens/sec: 8960.43\n",
      "step 231, loss: 3.7646889686584473, dt: 57.74 ms, tokens/sec: 8866.72\n",
      "step 232, loss: 4.125245571136475, dt: 56.62 ms, tokens/sec: 9042.76\n",
      "step 233, loss: 4.480816841125488, dt: 56.42 ms, tokens/sec: 9074.78\n",
      "step 234, loss: 5.057031631469727, dt: 57.00 ms, tokens/sec: 8982.28\n",
      "step 235, loss: 4.426527500152588, dt: 57.16 ms, tokens/sec: 8957.66\n",
      "step 236, loss: 4.907601356506348, dt: 56.31 ms, tokens/sec: 9093.30\n",
      "step 237, loss: 4.508140563964844, dt: 57.87 ms, tokens/sec: 8847.28\n",
      "step 238, loss: 4.607903480529785, dt: 56.04 ms, tokens/sec: 9135.66\n",
      "step 239, loss: 4.504072666168213, dt: 55.87 ms, tokens/sec: 9163.81\n",
      "step 240, loss: 4.1831374168396, dt: 56.89 ms, tokens/sec: 9000.61\n",
      "step 241, loss: 4.257007598876953, dt: 58.14 ms, tokens/sec: 8806.94\n",
      "step 242, loss: 4.18592643737793, dt: 56.18 ms, tokens/sec: 9113.91\n",
      "step 243, loss: 4.252901077270508, dt: 57.30 ms, tokens/sec: 8935.45\n",
      "step 244, loss: 4.0163068771362305, dt: 58.40 ms, tokens/sec: 8767.71\n",
      "step 245, loss: 4.078478813171387, dt: 56.49 ms, tokens/sec: 9063.06\n",
      "step 246, loss: 4.898512840270996, dt: 57.12 ms, tokens/sec: 8964.25\n",
      "step 247, loss: 4.619668483734131, dt: 56.72 ms, tokens/sec: 9027.44\n",
      "step 248, loss: 4.777988433837891, dt: 56.65 ms, tokens/sec: 9037.81\n",
      "step 249, loss: 4.661300182342529, dt: 56.59 ms, tokens/sec: 9047.67\n",
      "step 250, loss: 4.843186378479004, dt: 58.51 ms, tokens/sec: 8751.24\n",
      "step 251, loss: 5.462334156036377, dt: 59.52 ms, tokens/sec: 8602.29\n",
      "step 252, loss: 4.172987937927246, dt: 58.40 ms, tokens/sec: 8766.81\n",
      "step 253, loss: 4.3126220703125, dt: 58.47 ms, tokens/sec: 8756.59\n",
      "step 254, loss: 4.032940864562988, dt: 58.05 ms, tokens/sec: 8820.54\n",
      "step 255, loss: 3.886401653289795, dt: 61.02 ms, tokens/sec: 8390.31\n",
      "step 256, loss: 3.875687599182129, dt: 63.86 ms, tokens/sec: 8017.55\n",
      "step 257, loss: 3.8549304008483887, dt: 65.88 ms, tokens/sec: 7771.22\n",
      "step 258, loss: 5.460369110107422, dt: 67.21 ms, tokens/sec: 7617.35\n",
      "step 259, loss: 4.668155670166016, dt: 66.97 ms, tokens/sec: 7645.56\n",
      "step 260, loss: 4.661725997924805, dt: 67.11 ms, tokens/sec: 7629.37\n",
      "step 261, loss: 4.719273567199707, dt: 66.00 ms, tokens/sec: 7757.24\n",
      "step 262, loss: 4.4870805740356445, dt: 66.05 ms, tokens/sec: 7751.95\n",
      "step 263, loss: 4.426119804382324, dt: 63.88 ms, tokens/sec: 8015.42\n",
      "step 264, loss: 4.519231796264648, dt: 62.90 ms, tokens/sec: 8139.65\n",
      "step 265, loss: 4.310314178466797, dt: 57.76 ms, tokens/sec: 8864.27\n",
      "step 266, loss: 3.911046028137207, dt: 56.75 ms, tokens/sec: 9022.02\n",
      "step 267, loss: 4.359720706939697, dt: 55.97 ms, tokens/sec: 9148.27\n",
      "step 268, loss: 4.83347225189209, dt: 57.55 ms, tokens/sec: 8896.14\n",
      "step 269, loss: 4.788849830627441, dt: 56.32 ms, tokens/sec: 9090.57\n",
      "step 270, loss: 4.46038818359375, dt: 57.30 ms, tokens/sec: 8934.78\n",
      "step 271, loss: 4.148441314697266, dt: 57.25 ms, tokens/sec: 8943.08\n",
      "step 272, loss: 4.336940288543701, dt: 56.96 ms, tokens/sec: 8988.94\n",
      "step 273, loss: 4.4485673904418945, dt: 57.83 ms, tokens/sec: 8854.14\n",
      "step 274, loss: 4.643645286560059, dt: 59.11 ms, tokens/sec: 8662.39\n",
      "step 275, loss: 4.597422122955322, dt: 57.41 ms, tokens/sec: 8917.57\n",
      "step 276, loss: 4.719977378845215, dt: 55.43 ms, tokens/sec: 9236.53\n",
      "step 277, loss: 4.477747917175293, dt: 57.50 ms, tokens/sec: 8904.03\n",
      "step 278, loss: 4.434897422790527, dt: 56.20 ms, tokens/sec: 9110.93\n",
      "step 279, loss: 4.241227149963379, dt: 56.93 ms, tokens/sec: 8993.04\n",
      "step 280, loss: 4.269989013671875, dt: 56.24 ms, tokens/sec: 9104.37\n",
      "step 281, loss: 4.470276832580566, dt: 57.62 ms, tokens/sec: 8885.65\n",
      "step 282, loss: 4.382880210876465, dt: 56.59 ms, tokens/sec: 9047.87\n",
      "step 283, loss: 4.332569122314453, dt: 57.70 ms, tokens/sec: 8873.02\n",
      "step 284, loss: 4.485960960388184, dt: 58.09 ms, tokens/sec: 8813.30\n",
      "step 285, loss: 4.516845226287842, dt: 58.09 ms, tokens/sec: 8814.24\n",
      "step 286, loss: 4.248105049133301, dt: 58.32 ms, tokens/sec: 8779.29\n",
      "step 287, loss: 4.015233039855957, dt: 58.54 ms, tokens/sec: 8746.14\n",
      "step 288, loss: 4.252492427825928, dt: 57.05 ms, tokens/sec: 8974.59\n",
      "step 289, loss: 3.8376731872558594, dt: 57.97 ms, tokens/sec: 8832.04\n",
      "step 290, loss: 3.8567917346954346, dt: 58.09 ms, tokens/sec: 8814.20\n",
      "step 291, loss: 3.978996753692627, dt: 57.97 ms, tokens/sec: 8832.15\n",
      "step 292, loss: 3.736177921295166, dt: 58.14 ms, tokens/sec: 8806.29\n",
      "step 293, loss: 3.3713138103485107, dt: 58.60 ms, tokens/sec: 8736.46\n",
      "step 294, loss: 3.801298141479492, dt: 61.20 ms, tokens/sec: 8366.42\n",
      "step 295, loss: 3.8732962608337402, dt: 59.74 ms, tokens/sec: 8570.36\n",
      "step 296, loss: 4.029881477355957, dt: 59.07 ms, tokens/sec: 8667.21\n",
      "step 297, loss: 4.326351165771484, dt: 59.65 ms, tokens/sec: 8583.82\n",
      "step 298, loss: 3.465902328491211, dt: 58.88 ms, tokens/sec: 8695.81\n",
      "step 299, loss: 3.918386220932007, dt: 58.25 ms, tokens/sec: 8789.28\n",
      "step 300, loss: 3.687894821166992, dt: 58.48 ms, tokens/sec: 8755.88\n",
      "step 301, loss: 3.9964609146118164, dt: 58.47 ms, tokens/sec: 8756.34\n",
      "step 302, loss: 3.732900381088257, dt: 58.16 ms, tokens/sec: 8802.71\n",
      "step 303, loss: 3.8966245651245117, dt: 58.48 ms, tokens/sec: 8754.56\n",
      "step 304, loss: 3.8247792720794678, dt: 59.10 ms, tokens/sec: 8663.71\n",
      "step 305, loss: 3.5933382511138916, dt: 58.86 ms, tokens/sec: 8699.06\n",
      "step 306, loss: 3.951594829559326, dt: 59.78 ms, tokens/sec: 8564.62\n",
      "step 307, loss: 3.709710121154785, dt: 58.60 ms, tokens/sec: 8736.78\n",
      "step 308, loss: 4.004801273345947, dt: 59.21 ms, tokens/sec: 8647.91\n",
      "step 309, loss: 3.487227439880371, dt: 59.37 ms, tokens/sec: 8623.74\n",
      "step 310, loss: 4.229701995849609, dt: 57.98 ms, tokens/sec: 8830.00\n",
      "step 311, loss: 4.121118545532227, dt: 59.04 ms, tokens/sec: 8672.53\n",
      "step 312, loss: 3.9739506244659424, dt: 59.20 ms, tokens/sec: 8649.09\n",
      "step 313, loss: 3.6932029724121094, dt: 59.50 ms, tokens/sec: 8605.18\n",
      "step 314, loss: 3.2571003437042236, dt: 59.43 ms, tokens/sec: 8614.71\n",
      "step 315, loss: 3.4033737182617188, dt: 59.92 ms, tokens/sec: 8544.61\n",
      "step 316, loss: 3.4445464611053467, dt: 60.12 ms, tokens/sec: 8516.56\n",
      "step 317, loss: 3.583057165145874, dt: 60.26 ms, tokens/sec: 8496.10\n",
      "step 318, loss: 3.4577689170837402, dt: 60.18 ms, tokens/sec: 8507.92\n",
      "step 319, loss: 4.0122270584106445, dt: 59.91 ms, tokens/sec: 8546.01\n",
      "step 320, loss: 3.6490888595581055, dt: 60.66 ms, tokens/sec: 8440.77\n",
      "step 321, loss: 3.2136592864990234, dt: 59.96 ms, tokens/sec: 8539.59\n",
      "step 322, loss: 4.040395259857178, dt: 58.89 ms, tokens/sec: 8694.02\n",
      "step 323, loss: 3.818498134613037, dt: 59.95 ms, tokens/sec: 8540.57\n",
      "step 324, loss: 3.5747427940368652, dt: 59.23 ms, tokens/sec: 8643.84\n",
      "step 325, loss: 3.6857242584228516, dt: 59.89 ms, tokens/sec: 8548.59\n",
      "step 326, loss: 3.794828414916992, dt: 59.83 ms, tokens/sec: 8557.89\n",
      "step 327, loss: 3.7479403018951416, dt: 59.63 ms, tokens/sec: 8585.68\n",
      "step 328, loss: 3.48500394821167, dt: 58.60 ms, tokens/sec: 8737.49\n",
      "step 329, loss: 3.3794021606445312, dt: 59.85 ms, tokens/sec: 8555.17\n",
      "step 330, loss: 4.2562150955200195, dt: 59.69 ms, tokens/sec: 8577.21\n",
      "step 331, loss: 3.5985774993896484, dt: 59.49 ms, tokens/sec: 8607.05\n",
      "step 332, loss: 4.1421589851379395, dt: 59.81 ms, tokens/sec: 8560.01\n",
      "step 333, loss: 3.6884870529174805, dt: 59.61 ms, tokens/sec: 8589.63\n",
      "step 334, loss: 3.7688889503479004, dt: 60.39 ms, tokens/sec: 8478.19\n",
      "step 335, loss: 3.698493480682373, dt: 60.47 ms, tokens/sec: 8467.16\n",
      "step 336, loss: 3.2583670616149902, dt: 60.45 ms, tokens/sec: 8470.07\n",
      "step 337, loss: 3.3022730350494385, dt: 59.90 ms, tokens/sec: 8548.29\n",
      "step 338, loss: 3.842855930328369, dt: 58.99 ms, tokens/sec: 8679.89\n",
      "step 339, loss: 3.44527006149292, dt: 60.13 ms, tokens/sec: 8515.31\n",
      "step 340, loss: 3.2524311542510986, dt: 59.61 ms, tokens/sec: 8589.73\n",
      "step 341, loss: 3.706096887588501, dt: 59.83 ms, tokens/sec: 8557.86\n",
      "step 342, loss: 3.4891841411590576, dt: 59.78 ms, tokens/sec: 8565.20\n",
      "step 343, loss: 3.8587725162506104, dt: 59.85 ms, tokens/sec: 8555.10\n",
      "step 344, loss: 3.521665096282959, dt: 59.89 ms, tokens/sec: 8549.04\n",
      "step 345, loss: 3.868913173675537, dt: 59.50 ms, tokens/sec: 8605.46\n",
      "step 346, loss: 3.0816359519958496, dt: 59.64 ms, tokens/sec: 8585.13\n",
      "step 347, loss: 4.210505485534668, dt: 59.45 ms, tokens/sec: 8612.40\n",
      "step 348, loss: 3.651106119155884, dt: 63.15 ms, tokens/sec: 8108.18\n",
      "step 349, loss: 3.7944846153259277, dt: 68.08 ms, tokens/sec: 7520.12\n",
      "step 350, loss: 3.809624433517456, dt: 70.29 ms, tokens/sec: 7283.68\n",
      "step 351, loss: 3.373932361602783, dt: 67.86 ms, tokens/sec: 7545.49\n",
      "step 352, loss: 3.478470802307129, dt: 62.13 ms, tokens/sec: 8240.12\n",
      "step 353, loss: 3.45100736618042, dt: 60.16 ms, tokens/sec: 8510.48\n",
      "step 354, loss: 3.4064345359802246, dt: 59.70 ms, tokens/sec: 8576.52\n",
      "step 355, loss: 3.5780422687530518, dt: 65.28 ms, tokens/sec: 7842.91\n",
      "step 356, loss: 3.9322614669799805, dt: 66.41 ms, tokens/sec: 7709.56\n",
      "step 357, loss: 3.9606051445007324, dt: 69.26 ms, tokens/sec: 7392.01\n",
      "step 358, loss: 4.101075172424316, dt: 64.87 ms, tokens/sec: 7892.35\n",
      "step 359, loss: 3.401021718978882, dt: 60.36 ms, tokens/sec: 8483.01\n",
      "step 360, loss: 3.259354591369629, dt: 60.49 ms, tokens/sec: 8464.62\n",
      "step 361, loss: 3.742919445037842, dt: 65.56 ms, tokens/sec: 7809.29\n",
      "step 362, loss: 4.017179489135742, dt: 70.83 ms, tokens/sec: 7228.69\n",
      "step 363, loss: 4.4473795890808105, dt: 82.36 ms, tokens/sec: 6216.63\n",
      "step 364, loss: 3.9458541870117188, dt: 84.52 ms, tokens/sec: 6057.44\n",
      "step 365, loss: 4.355250835418701, dt: 79.40 ms, tokens/sec: 6448.36\n",
      "step 366, loss: 4.013181209564209, dt: 75.22 ms, tokens/sec: 6806.41\n",
      "step 367, loss: 4.107595443725586, dt: 74.69 ms, tokens/sec: 6854.90\n",
      "step 368, loss: 3.9424421787261963, dt: 78.17 ms, tokens/sec: 6549.58\n",
      "step 369, loss: 3.8158016204833984, dt: 83.72 ms, tokens/sec: 6115.77\n",
      "step 370, loss: 3.9573848247528076, dt: 81.43 ms, tokens/sec: 6287.61\n",
      "step 371, loss: 3.803773880004883, dt: 81.20 ms, tokens/sec: 6305.65\n",
      "step 372, loss: 3.841324806213379, dt: 86.39 ms, tokens/sec: 5926.69\n",
      "step 373, loss: 3.634413719177246, dt: 83.57 ms, tokens/sec: 6126.34\n",
      "step 374, loss: 3.7014236450195312, dt: 76.78 ms, tokens/sec: 6668.58\n",
      "step 375, loss: 4.403080940246582, dt: 79.24 ms, tokens/sec: 6461.22\n",
      "step 376, loss: 4.188457489013672, dt: 75.08 ms, tokens/sec: 6819.12\n",
      "step 377, loss: 4.313514709472656, dt: 65.36 ms, tokens/sec: 7833.53\n",
      "step 378, loss: 4.213514804840088, dt: 57.57 ms, tokens/sec: 8893.19\n",
      "step 379, loss: 4.367798328399658, dt: 56.40 ms, tokens/sec: 9077.35\n",
      "step 380, loss: 5.042442798614502, dt: 55.80 ms, tokens/sec: 9175.16\n",
      "step 381, loss: 3.78851318359375, dt: 57.88 ms, tokens/sec: 8845.13\n",
      "step 382, loss: 3.836028575897217, dt: 57.18 ms, tokens/sec: 8954.68\n",
      "step 383, loss: 3.599264144897461, dt: 56.48 ms, tokens/sec: 9064.36\n",
      "step 384, loss: 3.497882604598999, dt: 55.45 ms, tokens/sec: 9233.07\n",
      "step 385, loss: 3.507976770401001, dt: 57.23 ms, tokens/sec: 8945.72\n",
      "step 386, loss: 3.506927728652954, dt: 65.20 ms, tokens/sec: 7853.27\n",
      "step 387, loss: 4.72318172454834, dt: 61.98 ms, tokens/sec: 8260.60\n",
      "step 388, loss: 4.1186113357543945, dt: 64.20 ms, tokens/sec: 7975.32\n",
      "step 389, loss: 4.141169548034668, dt: 70.01 ms, tokens/sec: 7312.72\n",
      "step 390, loss: 4.243117332458496, dt: 70.48 ms, tokens/sec: 7264.36\n",
      "step 391, loss: 3.9979848861694336, dt: 65.93 ms, tokens/sec: 7765.43\n",
      "step 392, loss: 3.893401622772217, dt: 61.44 ms, tokens/sec: 8333.33\n",
      "step 393, loss: 3.976351261138916, dt: 56.60 ms, tokens/sec: 9046.42\n",
      "step 394, loss: 3.850606679916382, dt: 56.37 ms, tokens/sec: 9082.53\n",
      "step 395, loss: 3.5015244483947754, dt: 56.57 ms, tokens/sec: 9050.88\n",
      "step 396, loss: 3.9293906688690186, dt: 63.94 ms, tokens/sec: 8007.38\n",
      "step 397, loss: 4.353018760681152, dt: 62.53 ms, tokens/sec: 8188.72\n",
      "step 398, loss: 4.297945976257324, dt: 60.90 ms, tokens/sec: 8406.77\n",
      "step 399, loss: 4.0220184326171875, dt: 57.30 ms, tokens/sec: 8935.71\n",
      "step 400, loss: 3.7327077388763428, dt: 55.84 ms, tokens/sec: 9168.90\n",
      "step 401, loss: 3.8803210258483887, dt: 57.07 ms, tokens/sec: 8970.95\n",
      "step 402, loss: 3.9910800457000732, dt: 56.91 ms, tokens/sec: 8996.54\n",
      "step 403, loss: 4.199416160583496, dt: 55.96 ms, tokens/sec: 9149.29\n",
      "step 404, loss: 4.214791297912598, dt: 57.58 ms, tokens/sec: 8891.68\n",
      "step 405, loss: 4.275589466094971, dt: 55.45 ms, tokens/sec: 9234.03\n",
      "step 406, loss: 3.998228073120117, dt: 56.23 ms, tokens/sec: 9105.76\n",
      "step 407, loss: 3.95563006401062, dt: 56.54 ms, tokens/sec: 9055.23\n",
      "step 408, loss: 3.7800040245056152, dt: 57.04 ms, tokens/sec: 8976.27\n",
      "step 409, loss: 3.7976715564727783, dt: 57.65 ms, tokens/sec: 8881.13\n",
      "step 410, loss: 4.036179542541504, dt: 57.02 ms, tokens/sec: 8979.50\n",
      "step 411, loss: 3.932330846786499, dt: 57.83 ms, tokens/sec: 8854.00\n",
      "step 412, loss: 3.875576972961426, dt: 56.15 ms, tokens/sec: 9118.09\n",
      "step 413, loss: 4.087475776672363, dt: 57.92 ms, tokens/sec: 8840.26\n",
      "step 414, loss: 4.082061767578125, dt: 56.35 ms, tokens/sec: 9085.30\n",
      "step 415, loss: 3.825878143310547, dt: 55.77 ms, tokens/sec: 9180.22\n",
      "step 416, loss: 3.5703907012939453, dt: 57.13 ms, tokens/sec: 8961.51\n",
      "step 417, loss: 3.8274319171905518, dt: 57.13 ms, tokens/sec: 8961.85\n",
      "step 418, loss: 3.4513638019561768, dt: 56.08 ms, tokens/sec: 9129.18\n",
      "step 419, loss: 3.4688100814819336, dt: 56.21 ms, tokens/sec: 9108.81\n",
      "step 420, loss: 3.5923237800598145, dt: 56.58 ms, tokens/sec: 9049.47\n",
      "step 421, loss: 3.3614277839660645, dt: 57.94 ms, tokens/sec: 8837.38\n",
      "step 422, loss: 3.021040201187134, dt: 56.83 ms, tokens/sec: 9009.49\n",
      "step 423, loss: 3.3602936267852783, dt: 57.55 ms, tokens/sec: 8895.84\n",
      "step 424, loss: 3.4807021617889404, dt: 57.27 ms, tokens/sec: 8940.73\n",
      "step 425, loss: 3.63877010345459, dt: 57.30 ms, tokens/sec: 8935.45\n",
      "step 426, loss: 3.8105647563934326, dt: 55.70 ms, tokens/sec: 9192.25\n",
      "step 427, loss: 3.062192440032959, dt: 56.92 ms, tokens/sec: 8994.88\n",
      "step 428, loss: 3.5190911293029785, dt: 57.86 ms, tokens/sec: 8848.82\n",
      "step 429, loss: 3.381897449493408, dt: 58.12 ms, tokens/sec: 8809.32\n",
      "step 430, loss: 3.526247501373291, dt: 58.36 ms, tokens/sec: 8773.12\n",
      "step 431, loss: 3.319701910018921, dt: 58.68 ms, tokens/sec: 8725.00\n",
      "step 432, loss: 3.4675099849700928, dt: 58.58 ms, tokens/sec: 8740.34\n",
      "step 433, loss: 3.3765127658843994, dt: 57.15 ms, tokens/sec: 8959.01\n",
      "step 434, loss: 3.1711585521698, dt: 58.06 ms, tokens/sec: 8817.86\n",
      "step 435, loss: 3.473146915435791, dt: 57.40 ms, tokens/sec: 8920.31\n",
      "step 436, loss: 3.252957820892334, dt: 60.18 ms, tokens/sec: 8507.38\n",
      "step 437, loss: 3.54490065574646, dt: 58.50 ms, tokens/sec: 8751.95\n",
      "step 438, loss: 3.083160161972046, dt: 57.36 ms, tokens/sec: 8926.24\n",
      "step 439, loss: 3.610694408416748, dt: 56.61 ms, tokens/sec: 9044.97\n",
      "step 440, loss: 3.5751075744628906, dt: 57.04 ms, tokens/sec: 8975.37\n",
      "step 441, loss: 3.3503808975219727, dt: 57.24 ms, tokens/sec: 8945.43\n",
      "step 442, loss: 3.1636288166046143, dt: 61.83 ms, tokens/sec: 8280.13\n",
      "step 443, loss: 2.786273241043091, dt: 66.20 ms, tokens/sec: 7734.28\n",
      "step 444, loss: 2.8894171714782715, dt: 67.85 ms, tokens/sec: 7546.18\n",
      "step 445, loss: 2.992178201675415, dt: 67.40 ms, tokens/sec: 7596.23\n",
      "step 446, loss: 3.090177059173584, dt: 65.61 ms, tokens/sec: 7804.07\n",
      "step 447, loss: 2.9748566150665283, dt: 63.35 ms, tokens/sec: 8082.45\n",
      "step 448, loss: 3.498396873474121, dt: 60.87 ms, tokens/sec: 8411.51\n",
      "step 449, loss: 3.1305861473083496, dt: 59.48 ms, tokens/sec: 8607.91\n",
      "step 450, loss: 2.7591066360473633, dt: 58.54 ms, tokens/sec: 8746.00\n",
      "step 451, loss: 3.5606658458709717, dt: 59.78 ms, tokens/sec: 8564.86\n",
      "step 452, loss: 3.297393321990967, dt: 58.31 ms, tokens/sec: 8780.22\n",
      "step 453, loss: 3.087686538696289, dt: 58.60 ms, tokens/sec: 8736.57\n",
      "step 454, loss: 3.1837682723999023, dt: 59.07 ms, tokens/sec: 8668.40\n",
      "step 455, loss: 3.336559772491455, dt: 60.31 ms, tokens/sec: 8489.08\n",
      "step 456, loss: 3.220057487487793, dt: 60.01 ms, tokens/sec: 8531.92\n",
      "step 457, loss: 3.0123801231384277, dt: 60.05 ms, tokens/sec: 8525.82\n",
      "step 458, loss: 2.9823873043060303, dt: 60.83 ms, tokens/sec: 8416.62\n",
      "step 459, loss: 3.745823621749878, dt: 59.22 ms, tokens/sec: 8645.26\n",
      "step 460, loss: 3.1446385383605957, dt: 59.59 ms, tokens/sec: 8592.03\n",
      "step 461, loss: 3.6125688552856445, dt: 60.05 ms, tokens/sec: 8526.23\n",
      "step 462, loss: 3.1606550216674805, dt: 61.26 ms, tokens/sec: 8357.82\n",
      "step 463, loss: 3.2158782482147217, dt: 64.53 ms, tokens/sec: 7934.39\n",
      "step 464, loss: 3.229257106781006, dt: 67.25 ms, tokens/sec: 7613.73\n",
      "step 465, loss: 2.805253028869629, dt: 68.11 ms, tokens/sec: 7517.70\n",
      "step 466, loss: 2.878413677215576, dt: 68.02 ms, tokens/sec: 7527.21\n",
      "step 467, loss: 3.349963426589966, dt: 67.68 ms, tokens/sec: 7565.21\n",
      "step 468, loss: 2.9996817111968994, dt: 66.71 ms, tokens/sec: 7674.57\n",
      "step 469, loss: 2.828828811645508, dt: 65.59 ms, tokens/sec: 7806.45\n",
      "step 470, loss: 3.263871192932129, dt: 66.26 ms, tokens/sec: 7727.49\n",
      "step 471, loss: 3.038463592529297, dt: 63.85 ms, tokens/sec: 8018.92\n",
      "step 472, loss: 3.3423991203308105, dt: 57.81 ms, tokens/sec: 8856.48\n",
      "step 473, loss: 3.019740104675293, dt: 56.39 ms, tokens/sec: 9079.00\n",
      "step 474, loss: 3.3341636657714844, dt: 56.69 ms, tokens/sec: 9032.03\n",
      "step 475, loss: 2.631584405899048, dt: 56.58 ms, tokens/sec: 9049.92\n",
      "step 476, loss: 3.6351191997528076, dt: 55.32 ms, tokens/sec: 9255.56\n",
      "step 477, loss: 3.1850085258483887, dt: 56.65 ms, tokens/sec: 9037.62\n",
      "step 478, loss: 3.325082778930664, dt: 56.74 ms, tokens/sec: 9023.31\n",
      "step 479, loss: 3.3167147636413574, dt: 56.15 ms, tokens/sec: 9119.10\n",
      "step 480, loss: 2.856029987335205, dt: 57.57 ms, tokens/sec: 8893.49\n",
      "step 481, loss: 3.0489187240600586, dt: 56.51 ms, tokens/sec: 9060.96\n",
      "step 482, loss: 3.0397582054138184, dt: 58.61 ms, tokens/sec: 8735.36\n",
      "step 483, loss: 2.9642422199249268, dt: 56.60 ms, tokens/sec: 9045.46\n",
      "step 484, loss: 3.0599732398986816, dt: 57.16 ms, tokens/sec: 8957.59\n",
      "step 485, loss: 3.286545753479004, dt: 56.00 ms, tokens/sec: 9143.64\n",
      "step 486, loss: 3.3780760765075684, dt: 56.85 ms, tokens/sec: 9006.81\n",
      "step 487, loss: 3.523797035217285, dt: 56.42 ms, tokens/sec: 9075.28\n",
      "step 488, loss: 2.9071247577667236, dt: 57.11 ms, tokens/sec: 8965.33\n",
      "step 489, loss: 2.7327075004577637, dt: 56.94 ms, tokens/sec: 8992.40\n",
      "step 490, loss: 3.290393590927124, dt: 57.97 ms, tokens/sec: 8832.44\n",
      "step 491, loss: 3.4644365310668945, dt: 59.28 ms, tokens/sec: 8637.13\n",
      "step 492, loss: 3.7294845581054688, dt: 57.65 ms, tokens/sec: 8881.20\n",
      "step 493, loss: 3.4582691192626953, dt: 57.15 ms, tokens/sec: 8959.20\n",
      "step 494, loss: 3.7367031574249268, dt: 58.02 ms, tokens/sec: 8823.91\n",
      "step 495, loss: 3.427112340927124, dt: 57.37 ms, tokens/sec: 8924.35\n",
      "step 496, loss: 3.512810468673706, dt: 57.78 ms, tokens/sec: 8861.82\n",
      "step 497, loss: 3.31318998336792, dt: 57.44 ms, tokens/sec: 8913.49\n",
      "step 498, loss: 3.3183343410491943, dt: 56.70 ms, tokens/sec: 9030.14\n",
      "step 499, loss: 3.4784862995147705, dt: 57.09 ms, tokens/sec: 8968.70\n",
      "step 500, loss: 3.285876750946045, dt: 57.08 ms, tokens/sec: 8969.90\n",
      "step 501, loss: 3.3098325729370117, dt: 57.10 ms, tokens/sec: 8967.35\n",
      "step 502, loss: 3.174499750137329, dt: 57.35 ms, tokens/sec: 8928.24\n",
      "step 503, loss: 3.2173986434936523, dt: 57.41 ms, tokens/sec: 8918.12\n",
      "step 504, loss: 3.7325844764709473, dt: 57.11 ms, tokens/sec: 8964.58\n",
      "step 505, loss: 3.587472915649414, dt: 57.60 ms, tokens/sec: 8889.66\n",
      "step 506, loss: 3.7445993423461914, dt: 57.23 ms, tokens/sec: 8945.91\n",
      "step 507, loss: 3.614375591278076, dt: 57.02 ms, tokens/sec: 8979.91\n",
      "step 508, loss: 3.872279405593872, dt: 56.78 ms, tokens/sec: 9017.70\n",
      "step 509, loss: 4.472456932067871, dt: 57.93 ms, tokens/sec: 8838.44\n",
      "step 510, loss: 3.284905195236206, dt: 57.88 ms, tokens/sec: 8845.32\n",
      "step 511, loss: 3.3539605140686035, dt: 58.88 ms, tokens/sec: 8695.04\n",
      "step 512, loss: 3.097843647003174, dt: 58.69 ms, tokens/sec: 8723.83\n",
      "step 513, loss: 3.051203727722168, dt: 57.19 ms, tokens/sec: 8952.59\n",
      "step 514, loss: 3.1082770824432373, dt: 57.84 ms, tokens/sec: 8852.61\n",
      "step 515, loss: 3.117231845855713, dt: 57.80 ms, tokens/sec: 8857.83\n",
      "step 516, loss: 3.9836997985839844, dt: 56.22 ms, tokens/sec: 9107.42\n",
      "step 517, loss: 3.463944435119629, dt: 57.63 ms, tokens/sec: 8883.63\n",
      "step 518, loss: 3.5597105026245117, dt: 57.51 ms, tokens/sec: 8903.15\n",
      "step 519, loss: 3.637904644012451, dt: 57.38 ms, tokens/sec: 8922.79\n",
      "step 520, loss: 3.4263579845428467, dt: 59.50 ms, tokens/sec: 8605.46\n",
      "step 521, loss: 3.3096072673797607, dt: 60.43 ms, tokens/sec: 8472.07\n",
      "step 522, loss: 3.298215389251709, dt: 58.41 ms, tokens/sec: 8765.02\n",
      "step 523, loss: 3.2393431663513184, dt: 57.98 ms, tokens/sec: 8830.48\n",
      "step 524, loss: 2.9780874252319336, dt: 57.98 ms, tokens/sec: 8831.38\n",
      "step 525, loss: 3.339698314666748, dt: 57.43 ms, tokens/sec: 8915.05\n",
      "step 526, loss: 3.763335704803467, dt: 58.00 ms, tokens/sec: 8827.86\n",
      "step 527, loss: 3.6586947441101074, dt: 57.51 ms, tokens/sec: 8903.55\n",
      "step 528, loss: 3.459662437438965, dt: 56.22 ms, tokens/sec: 9107.42\n",
      "step 529, loss: 3.2376925945281982, dt: 58.81 ms, tokens/sec: 8705.54\n",
      "step 530, loss: 3.3559772968292236, dt: 58.48 ms, tokens/sec: 8755.59\n",
      "step 531, loss: 3.4001636505126953, dt: 60.10 ms, tokens/sec: 8518.58\n",
      "step 532, loss: 3.589409351348877, dt: 59.76 ms, tokens/sec: 8567.42\n",
      "step 533, loss: 3.604295253753662, dt: 59.06 ms, tokens/sec: 8669.87\n",
      "step 534, loss: 3.6663308143615723, dt: 59.26 ms, tokens/sec: 8639.73\n",
      "step 535, loss: 3.3996782302856445, dt: 58.87 ms, tokens/sec: 8696.98\n",
      "step 536, loss: 3.2962446212768555, dt: 58.29 ms, tokens/sec: 8784.10\n",
      "step 537, loss: 3.2795917987823486, dt: 56.88 ms, tokens/sec: 9001.37\n",
      "step 538, loss: 3.2187979221343994, dt: 58.00 ms, tokens/sec: 8827.75\n",
      "step 539, loss: 3.424856662750244, dt: 57.73 ms, tokens/sec: 8868.22\n",
      "step 540, loss: 3.2969532012939453, dt: 57.26 ms, tokens/sec: 8941.81\n",
      "step 541, loss: 3.327200174331665, dt: 56.95 ms, tokens/sec: 8990.82\n",
      "step 542, loss: 3.5362696647644043, dt: 59.44 ms, tokens/sec: 8613.02\n",
      "step 543, loss: 3.4693503379821777, dt: 63.86 ms, tokens/sec: 8016.95\n",
      "step 544, loss: 3.220425605773926, dt: 66.36 ms, tokens/sec: 7715.96\n",
      "step 545, loss: 3.0047831535339355, dt: 60.86 ms, tokens/sec: 8412.93\n",
      "step 546, loss: 3.303051233291626, dt: 57.99 ms, tokens/sec: 8829.10\n",
      "step 547, loss: 2.9549059867858887, dt: 59.16 ms, tokens/sec: 8654.78\n",
      "step 548, loss: 2.9392807483673096, dt: 65.35 ms, tokens/sec: 7835.33\n",
      "step 549, loss: 3.0551745891571045, dt: 61.72 ms, tokens/sec: 8295.67\n",
      "step 550, loss: 2.8656179904937744, dt: 63.40 ms, tokens/sec: 8075.86\n",
      "step 551, loss: 2.5645484924316406, dt: 59.08 ms, tokens/sec: 8666.34\n",
      "step 552, loss: 2.848785638809204, dt: 60.85 ms, tokens/sec: 8414.11\n",
      "step 553, loss: 2.932713508605957, dt: 65.37 ms, tokens/sec: 7832.07\n",
      "step 554, loss: 3.0688400268554688, dt: 62.31 ms, tokens/sec: 8217.39\n",
      "step 555, loss: 3.122819185256958, dt: 62.90 ms, tokens/sec: 8140.02\n",
      "step 556, loss: 2.5521557331085205, dt: 58.71 ms, tokens/sec: 8720.67\n",
      "step 557, loss: 2.929976463317871, dt: 56.50 ms, tokens/sec: 9061.95\n",
      "step 558, loss: 2.82554030418396, dt: 55.30 ms, tokens/sec: 9259.39\n",
      "step 559, loss: 2.9490864276885986, dt: 57.00 ms, tokens/sec: 8981.83\n",
      "step 560, loss: 2.7741947174072266, dt: 57.51 ms, tokens/sec: 8902.48\n",
      "step 561, loss: 2.9517393112182617, dt: 56.65 ms, tokens/sec: 9037.62\n",
      "step 562, loss: 2.8389859199523926, dt: 57.52 ms, tokens/sec: 8901.08\n",
      "step 563, loss: 2.6825368404388428, dt: 57.97 ms, tokens/sec: 8831.86\n",
      "step 564, loss: 2.9080734252929688, dt: 58.78 ms, tokens/sec: 8710.28\n",
      "step 565, loss: 2.697016716003418, dt: 58.72 ms, tokens/sec: 8720.07\n",
      "step 566, loss: 2.9187467098236084, dt: 59.31 ms, tokens/sec: 8632.16\n",
      "step 567, loss: 2.6134467124938965, dt: 58.09 ms, tokens/sec: 8813.34\n",
      "step 568, loss: 3.0276784896850586, dt: 59.33 ms, tokens/sec: 8629.84\n",
      "step 569, loss: 2.9146134853363037, dt: 58.41 ms, tokens/sec: 8765.92\n",
      "step 570, loss: 2.6867282390594482, dt: 60.70 ms, tokens/sec: 8434.67\n",
      "step 571, loss: 2.5944015979766846, dt: 66.57 ms, tokens/sec: 7691.40\n",
      "step 572, loss: 2.3098087310791016, dt: 67.75 ms, tokens/sec: 7557.65\n",
      "step 573, loss: 2.3528668880462646, dt: 67.97 ms, tokens/sec: 7532.73\n",
      "step 574, loss: 2.4876389503479004, dt: 66.88 ms, tokens/sec: 7655.61\n",
      "step 575, loss: 2.5674071311950684, dt: 64.94 ms, tokens/sec: 7883.71\n",
      "step 576, loss: 2.478428602218628, dt: 63.45 ms, tokens/sec: 8069.06\n",
      "step 577, loss: 2.863816022872925, dt: 61.87 ms, tokens/sec: 8274.86\n",
      "step 578, loss: 2.5999467372894287, dt: 62.80 ms, tokens/sec: 8152.23\n",
      "step 579, loss: 2.2943639755249023, dt: 59.16 ms, tokens/sec: 8655.09\n",
      "step 580, loss: 2.993535041809082, dt: 57.14 ms, tokens/sec: 8959.68\n",
      "step 581, loss: 2.7446179389953613, dt: 57.00 ms, tokens/sec: 8982.13\n",
      "step 582, loss: 2.5544400215148926, dt: 56.98 ms, tokens/sec: 8984.99\n",
      "step 583, loss: 2.615987777709961, dt: 57.40 ms, tokens/sec: 8919.08\n",
      "step 584, loss: 2.763437271118164, dt: 56.35 ms, tokens/sec: 9086.26\n",
      "step 585, loss: 2.7078211307525635, dt: 56.99 ms, tokens/sec: 8984.23\n",
      "step 586, loss: 2.5158944129943848, dt: 56.63 ms, tokens/sec: 9040.36\n",
      "step 587, loss: 2.4913060665130615, dt: 55.13 ms, tokens/sec: 9287.18\n",
      "step 588, loss: 3.0442097187042236, dt: 56.89 ms, tokens/sec: 9000.12\n",
      "step 589, loss: 2.5918798446655273, dt: 57.59 ms, tokens/sec: 8889.84\n",
      "step 590, loss: 2.9294991493225098, dt: 55.25 ms, tokens/sec: 9266.82\n",
      "step 591, loss: 2.6376421451568604, dt: 60.15 ms, tokens/sec: 8511.46\n",
      "step 592, loss: 2.626154899597168, dt: 56.08 ms, tokens/sec: 9129.29\n",
      "step 593, loss: 2.582209825515747, dt: 56.49 ms, tokens/sec: 9063.41\n",
      "step 594, loss: 2.2961928844451904, dt: 57.31 ms, tokens/sec: 8934.52\n",
      "step 595, loss: 2.295459270477295, dt: 56.29 ms, tokens/sec: 9095.11\n",
      "step 596, loss: 2.753140926361084, dt: 57.59 ms, tokens/sec: 8890.91\n",
      "step 597, loss: 2.447676181793213, dt: 56.17 ms, tokens/sec: 9115.19\n",
      "step 598, loss: 2.3333539962768555, dt: 55.66 ms, tokens/sec: 9198.23\n",
      "step 599, loss: 2.7245235443115234, dt: 56.70 ms, tokens/sec: 9029.53\n",
      "step 600, loss: 2.495544910430908, dt: 57.15 ms, tokens/sec: 8958.71\n",
      "step 601, loss: 2.7997138500213623, dt: 58.25 ms, tokens/sec: 8789.99\n",
      "step 602, loss: 2.524564743041992, dt: 57.10 ms, tokens/sec: 8967.46\n",
      "step 603, loss: 2.776369094848633, dt: 56.42 ms, tokens/sec: 9074.59\n",
      "step 604, loss: 2.1879725456237793, dt: 57.12 ms, tokens/sec: 8964.36\n",
      "step 605, loss: 3.043185234069824, dt: 57.31 ms, tokens/sec: 8933.11\n",
      "step 606, loss: 2.612166166305542, dt: 57.31 ms, tokens/sec: 8934.52\n",
      "step 607, loss: 2.730417251586914, dt: 56.89 ms, tokens/sec: 8999.07\n",
      "step 608, loss: 2.755108118057251, dt: 55.66 ms, tokens/sec: 9199.02\n",
      "step 609, loss: 2.30983829498291, dt: 57.03 ms, tokens/sec: 8977.70\n",
      "step 610, loss: 2.5313539505004883, dt: 56.71 ms, tokens/sec: 9028.09\n",
      "step 611, loss: 2.5192196369171143, dt: 59.95 ms, tokens/sec: 8540.60\n",
      "step 612, loss: 2.4197936058044434, dt: 57.22 ms, tokens/sec: 8947.92\n",
      "step 613, loss: 2.495087146759033, dt: 56.67 ms, tokens/sec: 9034.92\n",
      "step 614, loss: 2.664661407470703, dt: 56.21 ms, tokens/sec: 9109.16\n",
      "step 615, loss: 2.7927117347717285, dt: 56.84 ms, tokens/sec: 9007.45\n",
      "step 616, loss: 2.965332508087158, dt: 55.51 ms, tokens/sec: 9223.87\n",
      "step 617, loss: 2.4080214500427246, dt: 56.81 ms, tokens/sec: 9012.32\n",
      "step 618, loss: 2.2799854278564453, dt: 55.67 ms, tokens/sec: 9196.70\n",
      "step 619, loss: 2.7651476860046387, dt: 56.45 ms, tokens/sec: 9069.19\n",
      "step 620, loss: 2.9071383476257324, dt: 56.50 ms, tokens/sec: 9062.26\n",
      "step 621, loss: 3.0008325576782227, dt: 55.59 ms, tokens/sec: 9211.09\n",
      "step 622, loss: 2.812413215637207, dt: 55.75 ms, tokens/sec: 9183.52\n",
      "step 623, loss: 3.0540411472320557, dt: 56.77 ms, tokens/sec: 9019.18\n",
      "step 624, loss: 2.8821048736572266, dt: 56.71 ms, tokens/sec: 9027.74\n",
      "step 625, loss: 2.9134676456451416, dt: 55.77 ms, tokens/sec: 9179.91\n",
      "step 626, loss: 2.806119203567505, dt: 56.72 ms, tokens/sec: 9026.49\n",
      "step 627, loss: 2.7533109188079834, dt: 55.62 ms, tokens/sec: 9205.64\n",
      "step 628, loss: 2.903336524963379, dt: 56.71 ms, tokens/sec: 9028.09\n",
      "step 629, loss: 2.7235403060913086, dt: 56.23 ms, tokens/sec: 9105.10\n",
      "step 630, loss: 2.7389426231384277, dt: 55.50 ms, tokens/sec: 9224.55\n",
      "step 631, loss: 2.6611599922180176, dt: 56.63 ms, tokens/sec: 9041.31\n",
      "step 632, loss: 2.652245044708252, dt: 56.36 ms, tokens/sec: 9084.15\n",
      "step 633, loss: 3.0052201747894287, dt: 56.57 ms, tokens/sec: 9051.07\n",
      "step 634, loss: 2.824634552001953, dt: 56.40 ms, tokens/sec: 9078.50\n",
      "step 635, loss: 3.026857376098633, dt: 56.53 ms, tokens/sec: 9057.60\n",
      "step 636, loss: 2.9567370414733887, dt: 56.49 ms, tokens/sec: 9064.17\n",
      "step 637, loss: 3.2269206047058105, dt: 56.49 ms, tokens/sec: 9064.02\n",
      "step 638, loss: 3.773404598236084, dt: 55.18 ms, tokens/sec: 9278.71\n",
      "step 639, loss: 2.689764976501465, dt: 55.61 ms, tokens/sec: 9206.32\n",
      "step 640, loss: 2.7958548069000244, dt: 55.50 ms, tokens/sec: 9225.10\n",
      "step 641, loss: 2.55134916305542, dt: 55.23 ms, tokens/sec: 9270.50\n",
      "step 642, loss: 2.525089740753174, dt: 55.55 ms, tokens/sec: 9216.59\n",
      "step 643, loss: 2.6376447677612305, dt: 56.62 ms, tokens/sec: 9043.22\n",
      "step 644, loss: 2.6741998195648193, dt: 56.32 ms, tokens/sec: 9090.11\n",
      "step 645, loss: 3.104665756225586, dt: 56.20 ms, tokens/sec: 9109.66\n",
      "step 646, loss: 2.777914524078369, dt: 56.66 ms, tokens/sec: 9036.56\n",
      "step 647, loss: 2.90083909034729, dt: 56.47 ms, tokens/sec: 9066.16\n",
      "step 648, loss: 2.9397382736206055, dt: 56.73 ms, tokens/sec: 9024.75\n",
      "step 649, loss: 2.7578728199005127, dt: 55.38 ms, tokens/sec: 9244.56\n",
      "step 650, loss: 2.6732778549194336, dt: 55.26 ms, tokens/sec: 9265.78\n",
      "step 651, loss: 2.5960779190063477, dt: 56.05 ms, tokens/sec: 9134.07\n",
      "step 652, loss: 2.583122730255127, dt: 58.14 ms, tokens/sec: 8806.03\n",
      "step 653, loss: 2.4600374698638916, dt: 56.94 ms, tokens/sec: 8992.10\n",
      "step 654, loss: 2.7255187034606934, dt: 55.64 ms, tokens/sec: 9202.17\n",
      "step 655, loss: 3.0319955348968506, dt: 57.73 ms, tokens/sec: 8868.37\n",
      "step 656, loss: 2.9265005588531494, dt: 55.72 ms, tokens/sec: 9189.45\n",
      "step 657, loss: 2.8496594429016113, dt: 56.72 ms, tokens/sec: 9027.59\n",
      "step 658, loss: 2.6264381408691406, dt: 55.61 ms, tokens/sec: 9207.14\n",
      "step 659, loss: 2.730820655822754, dt: 56.64 ms, tokens/sec: 9040.13\n",
      "step 660, loss: 2.7024264335632324, dt: 56.77 ms, tokens/sec: 9019.18\n",
      "step 661, loss: 2.879425287246704, dt: 56.74 ms, tokens/sec: 9023.65\n",
      "step 662, loss: 2.8734779357910156, dt: 56.42 ms, tokens/sec: 9075.47\n",
      "step 663, loss: 2.8880012035369873, dt: 59.22 ms, tokens/sec: 8646.34\n",
      "step 664, loss: 2.6705362796783447, dt: 55.61 ms, tokens/sec: 9206.95\n",
      "step 665, loss: 2.619321346282959, dt: 57.57 ms, tokens/sec: 8894.00\n",
      "step 666, loss: 2.598830223083496, dt: 55.64 ms, tokens/sec: 9201.50\n",
      "step 667, loss: 2.562408447265625, dt: 56.77 ms, tokens/sec: 9018.08\n",
      "step 668, loss: 2.730743885040283, dt: 56.50 ms, tokens/sec: 9061.49\n",
      "step 669, loss: 2.6691625118255615, dt: 56.90 ms, tokens/sec: 8997.97\n",
      "step 670, loss: 2.571992874145508, dt: 56.82 ms, tokens/sec: 9011.08\n",
      "step 671, loss: 2.8050551414489746, dt: 55.40 ms, tokens/sec: 9241.54\n",
      "step 672, loss: 2.8637237548828125, dt: 58.03 ms, tokens/sec: 8823.00\n",
      "step 673, loss: 2.5933589935302734, dt: 58.22 ms, tokens/sec: 8794.53\n",
      "step 674, loss: 2.4002737998962402, dt: 57.22 ms, tokens/sec: 8948.11\n",
      "step 675, loss: 2.6000499725341797, dt: 56.60 ms, tokens/sec: 9045.16\n",
      "step 676, loss: 2.335297107696533, dt: 57.98 ms, tokens/sec: 8830.62\n",
      "step 677, loss: 2.3667287826538086, dt: 57.47 ms, tokens/sec: 8909.76\n",
      "step 678, loss: 2.483006000518799, dt: 57.26 ms, tokens/sec: 8942.15\n",
      "step 679, loss: 2.3180413246154785, dt: 58.26 ms, tokens/sec: 8788.05\n",
      "step 680, loss: 2.110130786895752, dt: 59.43 ms, tokens/sec: 8614.92\n",
      "step 681, loss: 2.2630038261413574, dt: 64.70 ms, tokens/sec: 7913.78\n",
      "step 682, loss: 2.382385015487671, dt: 69.54 ms, tokens/sec: 7362.46\n",
      "step 683, loss: 2.4140539169311523, dt: 76.57 ms, tokens/sec: 6686.96\n",
      "step 684, loss: 2.4127049446105957, dt: 76.18 ms, tokens/sec: 6720.93\n",
      "step 685, loss: 2.0219340324401855, dt: 75.08 ms, tokens/sec: 6819.60\n",
      "step 686, loss: 2.3067970275878906, dt: 72.54 ms, tokens/sec: 7058.50\n",
      "step 687, loss: 2.2213470935821533, dt: 71.41 ms, tokens/sec: 7169.89\n",
      "step 688, loss: 2.2434134483337402, dt: 68.87 ms, tokens/sec: 7434.39\n",
      "step 689, loss: 2.1460256576538086, dt: 66.42 ms, tokens/sec: 7708.60\n",
      "step 690, loss: 2.2881765365600586, dt: 63.61 ms, tokens/sec: 8048.56\n",
      "step 691, loss: 2.271312713623047, dt: 59.72 ms, tokens/sec: 8573.51\n",
      "step 692, loss: 2.1154446601867676, dt: 58.01 ms, tokens/sec: 8825.94\n",
      "step 693, loss: 2.3081462383270264, dt: 58.03 ms, tokens/sec: 8823.76\n",
      "step 694, loss: 2.115710735321045, dt: 56.08 ms, tokens/sec: 9129.18\n",
      "step 695, loss: 2.2862582206726074, dt: 56.75 ms, tokens/sec: 9021.41\n",
      "step 696, loss: 2.032409429550171, dt: 57.34 ms, tokens/sec: 8928.87\n",
      "step 697, loss: 2.3478357791900635, dt: 56.70 ms, tokens/sec: 9030.44\n",
      "step 698, loss: 2.2797470092773438, dt: 57.08 ms, tokens/sec: 8969.23\n",
      "step 699, loss: 2.044212818145752, dt: 56.09 ms, tokens/sec: 9127.55\n",
      "step 700, loss: 1.960301160812378, dt: 57.51 ms, tokens/sec: 8903.55\n",
      "step 701, loss: 1.7199006080627441, dt: 57.39 ms, tokens/sec: 8921.23\n",
      "step 702, loss: 1.8243461847305298, dt: 59.53 ms, tokens/sec: 8601.15\n",
      "step 703, loss: 1.8486428260803223, dt: 57.84 ms, tokens/sec: 8852.03\n",
      "step 704, loss: 1.9371514320373535, dt: 56.81 ms, tokens/sec: 9012.85\n",
      "step 705, loss: 1.852895736694336, dt: 57.62 ms, tokens/sec: 8886.31\n",
      "step 706, loss: 2.238809823989868, dt: 57.05 ms, tokens/sec: 8974.77\n",
      "step 707, loss: 2.036710739135742, dt: 57.08 ms, tokens/sec: 8969.41\n",
      "step 708, loss: 1.800220012664795, dt: 56.73 ms, tokens/sec: 9024.86\n",
      "step 709, loss: 2.276550769805908, dt: 57.12 ms, tokens/sec: 8962.82\n",
      "step 710, loss: 2.0600671768188477, dt: 56.93 ms, tokens/sec: 8993.79\n",
      "step 711, loss: 2.027143955230713, dt: 61.07 ms, tokens/sec: 8383.92\n",
      "step 712, loss: 2.127668857574463, dt: 66.91 ms, tokens/sec: 7651.96\n",
      "step 713, loss: 2.200246810913086, dt: 66.14 ms, tokens/sec: 7741.30\n",
      "step 714, loss: 2.1607136726379395, dt: 61.58 ms, tokens/sec: 8313.72\n",
      "step 715, loss: 1.8990604877471924, dt: 58.13 ms, tokens/sec: 8808.60\n",
      "step 716, loss: 1.9313292503356934, dt: 56.75 ms, tokens/sec: 9021.68\n",
      "step 717, loss: 2.3585164546966553, dt: 57.49 ms, tokens/sec: 8905.88\n",
      "step 718, loss: 2.051403522491455, dt: 56.46 ms, tokens/sec: 9068.69\n",
      "step 719, loss: 2.2623860836029053, dt: 56.32 ms, tokens/sec: 9091.49\n",
      "step 720, loss: 2.029615879058838, dt: 57.40 ms, tokens/sec: 8919.42\n",
      "step 721, loss: 2.021218776702881, dt: 57.44 ms, tokens/sec: 8913.16\n",
      "step 722, loss: 2.036552906036377, dt: 58.59 ms, tokens/sec: 8739.02\n",
      "step 723, loss: 1.7662644386291504, dt: 57.73 ms, tokens/sec: 8869.28\n",
      "step 724, loss: 1.7971409559249878, dt: 57.66 ms, tokens/sec: 8880.28\n",
      "step 725, loss: 2.1265344619750977, dt: 59.56 ms, tokens/sec: 8596.92\n",
      "step 726, loss: 1.9387781620025635, dt: 59.54 ms, tokens/sec: 8598.67\n",
      "step 727, loss: 1.7469359636306763, dt: 58.87 ms, tokens/sec: 8697.72\n",
      "step 728, loss: 2.1146106719970703, dt: 58.16 ms, tokens/sec: 8803.11\n",
      "step 729, loss: 1.9090003967285156, dt: 58.62 ms, tokens/sec: 8733.66\n",
      "step 730, loss: 2.198741912841797, dt: 56.86 ms, tokens/sec: 9003.78\n",
      "step 731, loss: 1.953601598739624, dt: 57.54 ms, tokens/sec: 8898.79\n",
      "step 732, loss: 2.1236448287963867, dt: 58.33 ms, tokens/sec: 8778.10\n",
      "step 733, loss: 1.621870756149292, dt: 58.22 ms, tokens/sec: 8794.13\n",
      "step 734, loss: 2.337169885635376, dt: 58.35 ms, tokens/sec: 8774.44\n",
      "step 735, loss: 1.9311707019805908, dt: 57.55 ms, tokens/sec: 8895.96\n",
      "step 736, loss: 2.1007721424102783, dt: 58.17 ms, tokens/sec: 8801.67\n",
      "step 737, loss: 2.0601134300231934, dt: 58.17 ms, tokens/sec: 8802.53\n",
      "step 738, loss: 1.7532739639282227, dt: 58.21 ms, tokens/sec: 8795.47\n",
      "step 739, loss: 1.968684196472168, dt: 57.90 ms, tokens/sec: 8843.42\n",
      "step 740, loss: 1.9971888065338135, dt: 57.88 ms, tokens/sec: 8845.39\n",
      "step 741, loss: 1.9665645360946655, dt: 57.07 ms, tokens/sec: 8970.99\n",
      "step 742, loss: 1.923417329788208, dt: 57.74 ms, tokens/sec: 8867.05\n",
      "step 743, loss: 2.055368423461914, dt: 57.87 ms, tokens/sec: 8847.76\n",
      "step 744, loss: 2.183440923690796, dt: 63.66 ms, tokens/sec: 8043.22\n",
      "step 745, loss: 2.231064558029175, dt: 66.25 ms, tokens/sec: 7728.38\n",
      "step 746, loss: 1.8997149467468262, dt: 63.54 ms, tokens/sec: 8057.40\n",
      "step 747, loss: 1.7431224584579468, dt: 57.55 ms, tokens/sec: 8897.25\n",
      "step 748, loss: 2.1513137817382812, dt: 56.96 ms, tokens/sec: 8988.45\n",
      "step 749, loss: 2.197535991668701, dt: 56.51 ms, tokens/sec: 9060.65\n",
      "step 750, loss: 2.2326862812042236, dt: 55.84 ms, tokens/sec: 9169.37\n",
      "step 751, loss: 2.1766395568847656, dt: 57.25 ms, tokens/sec: 8942.89\n",
      "step 752, loss: 2.3341286182403564, dt: 57.98 ms, tokens/sec: 8831.06\n",
      "step 753, loss: 2.151022434234619, dt: 57.03 ms, tokens/sec: 8977.25\n",
      "step 754, loss: 2.214176654815674, dt: 55.72 ms, tokens/sec: 9188.47\n",
      "step 755, loss: 2.1506776809692383, dt: 56.31 ms, tokens/sec: 9092.49\n",
      "step 756, loss: 2.134826421737671, dt: 55.59 ms, tokens/sec: 9210.82\n",
      "step 757, loss: 2.312333345413208, dt: 57.02 ms, tokens/sec: 8979.46\n",
      "step 758, loss: 2.058845281600952, dt: 56.59 ms, tokens/sec: 9047.52\n",
      "step 759, loss: 2.0632896423339844, dt: 58.69 ms, tokens/sec: 8723.79\n",
      "step 760, loss: 2.0787415504455566, dt: 60.21 ms, tokens/sec: 8503.57\n",
      "step 761, loss: 2.0870766639709473, dt: 61.93 ms, tokens/sec: 8267.15\n",
      "step 762, loss: 2.320155620574951, dt: 61.98 ms, tokens/sec: 8260.60\n",
      "step 763, loss: 2.2076735496520996, dt: 62.70 ms, tokens/sec: 8165.46\n",
      "step 764, loss: 2.2657370567321777, dt: 61.31 ms, tokens/sec: 8351.29\n",
      "step 765, loss: 2.193018913269043, dt: 62.06 ms, tokens/sec: 8249.43\n",
      "step 766, loss: 2.345795154571533, dt: 61.24 ms, tokens/sec: 8360.00\n",
      "step 767, loss: 2.821920394897461, dt: 60.61 ms, tokens/sec: 8447.18\n",
      "step 768, loss: 2.0698070526123047, dt: 61.68 ms, tokens/sec: 8300.32\n",
      "step 769, loss: 2.198991298675537, dt: 59.88 ms, tokens/sec: 8550.30\n",
      "step 770, loss: 1.9770197868347168, dt: 60.55 ms, tokens/sec: 8455.26\n",
      "step 771, loss: 2.0392651557922363, dt: 59.95 ms, tokens/sec: 8541.01\n",
      "step 772, loss: 2.0285227298736572, dt: 60.24 ms, tokens/sec: 8499.47\n",
      "step 773, loss: 2.1099514961242676, dt: 55.77 ms, tokens/sec: 9179.75\n",
      "step 774, loss: 2.169290781021118, dt: 55.14 ms, tokens/sec: 9286.13\n",
      "step 775, loss: 2.0350825786590576, dt: 56.91 ms, tokens/sec: 8996.84\n",
      "step 776, loss: 2.1749658584594727, dt: 57.31 ms, tokens/sec: 8933.74\n",
      "step 777, loss: 2.156036853790283, dt: 58.42 ms, tokens/sec: 8763.67\n",
      "step 778, loss: 2.056544303894043, dt: 57.51 ms, tokens/sec: 8903.41\n",
      "step 779, loss: 1.9781262874603271, dt: 57.84 ms, tokens/sec: 8851.84\n",
      "step 780, loss: 1.991720199584961, dt: 61.69 ms, tokens/sec: 8299.68\n",
      "step 781, loss: 1.9252524375915527, dt: 64.40 ms, tokens/sec: 7950.17\n",
      "step 782, loss: 1.8370068073272705, dt: 64.86 ms, tokens/sec: 7894.17\n",
      "step 783, loss: 1.9767022132873535, dt: 62.76 ms, tokens/sec: 8157.52\n",
      "step 784, loss: 2.2605390548706055, dt: 61.23 ms, tokens/sec: 8362.05\n",
      "step 785, loss: 2.1504054069519043, dt: 61.04 ms, tokens/sec: 8387.49\n",
      "step 786, loss: 2.177053928375244, dt: 61.66 ms, tokens/sec: 8304.14\n",
      "step 787, loss: 1.960450291633606, dt: 64.60 ms, tokens/sec: 7925.70\n",
      "step 788, loss: 2.0383036136627197, dt: 66.55 ms, tokens/sec: 7693.46\n",
      "step 789, loss: 2.0776047706604004, dt: 67.58 ms, tokens/sec: 7576.21\n",
      "step 790, loss: 2.1993067264556885, dt: 66.91 ms, tokens/sec: 7652.15\n",
      "step 791, loss: 2.1351988315582275, dt: 66.48 ms, tokens/sec: 7701.90\n",
      "step 792, loss: 2.076779842376709, dt: 67.30 ms, tokens/sec: 7607.72\n",
      "step 793, loss: 1.9790852069854736, dt: 66.12 ms, tokens/sec: 7743.81\n",
      "step 794, loss: 1.8897044658660889, dt: 65.83 ms, tokens/sec: 7777.86\n",
      "step 795, loss: 1.87394380569458, dt: 64.16 ms, tokens/sec: 7980.63\n",
      "step 796, loss: 1.916534185409546, dt: 59.64 ms, tokens/sec: 8584.13\n",
      "step 797, loss: 1.9449671506881714, dt: 56.93 ms, tokens/sec: 8993.49\n",
      "step 798, loss: 1.9247024059295654, dt: 56.94 ms, tokens/sec: 8991.27\n",
      "step 799, loss: 1.9302557706832886, dt: 56.10 ms, tokens/sec: 9125.92\n",
      "step 800, loss: 2.0533766746520996, dt: 55.81 ms, tokens/sec: 9174.61\n",
      "step 801, loss: 2.0810275077819824, dt: 56.94 ms, tokens/sec: 8992.40\n",
      "step 802, loss: 1.815997838973999, dt: 57.06 ms, tokens/sec: 8973.16\n",
      "step 803, loss: 1.7736074924468994, dt: 55.86 ms, tokens/sec: 9165.76\n",
      "step 804, loss: 1.9323601722717285, dt: 56.06 ms, tokens/sec: 9133.88\n",
      "step 805, loss: 1.8171371221542358, dt: 56.62 ms, tokens/sec: 9042.07\n",
      "step 806, loss: 1.788895606994629, dt: 55.87 ms, tokens/sec: 9163.46\n",
      "step 807, loss: 1.8193645477294922, dt: 57.51 ms, tokens/sec: 8902.34\n",
      "step 808, loss: 1.8470991849899292, dt: 55.55 ms, tokens/sec: 9216.31\n",
      "step 809, loss: 1.595946192741394, dt: 55.65 ms, tokens/sec: 9200.87\n",
      "step 810, loss: 1.7510952949523926, dt: 57.20 ms, tokens/sec: 8950.27\n",
      "step 811, loss: 1.7961477041244507, dt: 56.13 ms, tokens/sec: 9121.69\n",
      "step 812, loss: 1.8294320106506348, dt: 60.53 ms, tokens/sec: 8458.62\n",
      "step 813, loss: 1.7950307130813599, dt: 56.79 ms, tokens/sec: 9015.99\n",
      "step 814, loss: 1.5615694522857666, dt: 56.15 ms, tokens/sec: 9117.82\n",
      "step 815, loss: 1.6806732416152954, dt: 56.83 ms, tokens/sec: 9009.30\n",
      "step 816, loss: 1.631281852722168, dt: 55.36 ms, tokens/sec: 9249.18\n",
      "step 817, loss: 1.6072666645050049, dt: 56.17 ms, tokens/sec: 9115.03\n",
      "step 818, loss: 1.6250388622283936, dt: 56.03 ms, tokens/sec: 9138.62\n",
      "step 819, loss: 1.653159737586975, dt: 57.16 ms, tokens/sec: 8957.03\n",
      "step 820, loss: 1.6853975057601929, dt: 57.16 ms, tokens/sec: 8957.18\n",
      "step 821, loss: 1.5772104263305664, dt: 57.35 ms, tokens/sec: 8927.65\n",
      "step 822, loss: 1.6673681735992432, dt: 58.56 ms, tokens/sec: 8742.58\n",
      "step 823, loss: 1.4805856943130493, dt: 58.00 ms, tokens/sec: 8827.72\n",
      "step 824, loss: 1.60475754737854, dt: 57.48 ms, tokens/sec: 8907.43\n",
      "step 825, loss: 1.4730238914489746, dt: 58.10 ms, tokens/sec: 8812.25\n",
      "step 826, loss: 1.7673431634902954, dt: 58.05 ms, tokens/sec: 8819.52\n",
      "step 827, loss: 1.6882542371749878, dt: 57.70 ms, tokens/sec: 8873.35\n",
      "step 828, loss: 1.5103325843811035, dt: 58.34 ms, tokens/sec: 8776.42\n",
      "step 829, loss: 1.4002890586853027, dt: 57.95 ms, tokens/sec: 8835.05\n",
      "step 830, loss: 1.2345221042633057, dt: 58.21 ms, tokens/sec: 8796.04\n",
      "step 831, loss: 1.2823410034179688, dt: 57.99 ms, tokens/sec: 8829.39\n",
      "step 832, loss: 1.3696871995925903, dt: 59.34 ms, tokens/sec: 8628.69\n",
      "step 833, loss: 1.3481130599975586, dt: 57.89 ms, tokens/sec: 8844.33\n",
      "step 834, loss: 1.3478838205337524, dt: 58.44 ms, tokens/sec: 8761.23\n",
      "step 835, loss: 1.5158047676086426, dt: 58.24 ms, tokens/sec: 8791.07\n",
      "step 836, loss: 1.4593992233276367, dt: 58.42 ms, tokens/sec: 8763.70\n",
      "step 837, loss: 1.2891802787780762, dt: 57.96 ms, tokens/sec: 8833.64\n",
      "step 838, loss: 1.6446603536605835, dt: 57.38 ms, tokens/sec: 8923.61\n",
      "step 839, loss: 1.4673216342926025, dt: 57.58 ms, tokens/sec: 8891.24\n",
      "step 840, loss: 1.4115328788757324, dt: 56.46 ms, tokens/sec: 9067.73\n",
      "step 841, loss: 1.411452293395996, dt: 57.15 ms, tokens/sec: 8958.67\n",
      "step 842, loss: 1.5408577919006348, dt: 57.37 ms, tokens/sec: 8924.68\n",
      "step 843, loss: 1.6319485902786255, dt: 61.36 ms, tokens/sec: 8344.08\n",
      "step 844, loss: 1.3891983032226562, dt: 62.07 ms, tokens/sec: 8249.11\n",
      "step 845, loss: 1.3798067569732666, dt: 65.33 ms, tokens/sec: 7836.90\n",
      "step 846, loss: 1.609825849533081, dt: 60.97 ms, tokens/sec: 8397.17\n",
      "step 847, loss: 1.4501371383666992, dt: 57.62 ms, tokens/sec: 8885.35\n",
      "step 848, loss: 1.617222547531128, dt: 56.91 ms, tokens/sec: 8996.35\n",
      "step 849, loss: 1.4664373397827148, dt: 56.22 ms, tokens/sec: 9107.22\n",
      "step 850, loss: 1.4257677793502808, dt: 56.77 ms, tokens/sec: 9018.87\n",
      "step 851, loss: 1.483635425567627, dt: 56.97 ms, tokens/sec: 8987.05\n",
      "step 852, loss: 1.2948944568634033, dt: 57.17 ms, tokens/sec: 8956.39\n",
      "step 853, loss: 1.2565653324127197, dt: 56.09 ms, tokens/sec: 9128.32\n",
      "step 854, loss: 1.5014728307724, dt: 56.15 ms, tokens/sec: 9117.63\n",
      "step 855, loss: 1.3137438297271729, dt: 56.71 ms, tokens/sec: 9028.24\n",
      "step 856, loss: 1.1998077630996704, dt: 57.73 ms, tokens/sec: 8868.11\n",
      "step 857, loss: 1.4370989799499512, dt: 56.65 ms, tokens/sec: 9038.12\n",
      "step 858, loss: 1.3130828142166138, dt: 58.33 ms, tokens/sec: 8777.06\n",
      "step 859, loss: 1.5130670070648193, dt: 58.09 ms, tokens/sec: 8814.35\n",
      "step 860, loss: 1.364018440246582, dt: 57.72 ms, tokens/sec: 8870.38\n",
      "step 861, loss: 1.4966084957122803, dt: 57.98 ms, tokens/sec: 8831.06\n",
      "step 862, loss: 1.1715997457504272, dt: 56.89 ms, tokens/sec: 9000.28\n",
      "step 863, loss: 1.6245427131652832, dt: 58.72 ms, tokens/sec: 8719.19\n",
      "step 864, loss: 1.3422996997833252, dt: 58.72 ms, tokens/sec: 8719.36\n",
      "step 865, loss: 1.4768080711364746, dt: 59.15 ms, tokens/sec: 8655.33\n",
      "step 866, loss: 1.5070946216583252, dt: 59.18 ms, tokens/sec: 8651.71\n",
      "step 867, loss: 1.3141299486160278, dt: 58.82 ms, tokens/sec: 8704.49\n",
      "step 868, loss: 1.3588309288024902, dt: 56.89 ms, tokens/sec: 9000.43\n",
      "step 869, loss: 1.359304666519165, dt: 57.86 ms, tokens/sec: 8849.69\n",
      "step 870, loss: 1.3780641555786133, dt: 58.11 ms, tokens/sec: 8810.41\n",
      "step 871, loss: 1.3595268726348877, dt: 60.78 ms, tokens/sec: 8423.39\n",
      "step 872, loss: 1.4735033512115479, dt: 59.44 ms, tokens/sec: 8613.60\n",
      "step 873, loss: 1.5008574724197388, dt: 58.12 ms, tokens/sec: 8809.21\n",
      "step 874, loss: 1.5888327360153198, dt: 57.38 ms, tokens/sec: 8922.49\n",
      "step 875, loss: 1.3234513998031616, dt: 57.07 ms, tokens/sec: 8972.07\n",
      "step 876, loss: 1.2838754653930664, dt: 57.18 ms, tokens/sec: 8954.23\n",
      "step 877, loss: 1.546834945678711, dt: 57.87 ms, tokens/sec: 8847.90\n",
      "step 878, loss: 1.5177479982376099, dt: 57.40 ms, tokens/sec: 8919.20\n",
      "step 879, loss: 1.5581220388412476, dt: 58.83 ms, tokens/sec: 8702.62\n",
      "step 880, loss: 1.4927884340286255, dt: 58.04 ms, tokens/sec: 8821.81\n",
      "step 881, loss: 1.5503015518188477, dt: 58.00 ms, tokens/sec: 8827.61\n",
      "step 882, loss: 1.4718501567840576, dt: 57.15 ms, tokens/sec: 8959.53\n",
      "step 883, loss: 1.5624642372131348, dt: 57.33 ms, tokens/sec: 8931.18\n",
      "step 884, loss: 1.457197904586792, dt: 58.07 ms, tokens/sec: 8817.10\n",
      "step 885, loss: 1.4385876655578613, dt: 58.62 ms, tokens/sec: 8734.22\n",
      "step 886, loss: 1.5522911548614502, dt: 58.83 ms, tokens/sec: 8702.72\n",
      "step 887, loss: 1.4462511539459229, dt: 62.64 ms, tokens/sec: 8173.70\n",
      "step 888, loss: 1.3908753395080566, dt: 65.03 ms, tokens/sec: 7873.19\n",
      "step 889, loss: 1.4189772605895996, dt: 65.18 ms, tokens/sec: 7855.68\n",
      "step 890, loss: 1.4353837966918945, dt: 63.03 ms, tokens/sec: 8122.84\n",
      "step 891, loss: 1.6071232557296753, dt: 57.83 ms, tokens/sec: 8853.08\n",
      "step 892, loss: 1.5091478824615479, dt: 59.98 ms, tokens/sec: 8535.61\n",
      "step 893, loss: 1.4596149921417236, dt: 63.06 ms, tokens/sec: 8118.63\n",
      "step 894, loss: 1.4402523040771484, dt: 63.73 ms, tokens/sec: 8033.65\n",
      "step 895, loss: 1.6284656524658203, dt: 66.89 ms, tokens/sec: 7654.14\n",
      "step 896, loss: 1.922797679901123, dt: 68.92 ms, tokens/sec: 7428.81\n",
      "step 897, loss: 1.4104703664779663, dt: 68.10 ms, tokens/sec: 7518.49\n",
      "step 898, loss: 1.5639312267303467, dt: 67.66 ms, tokens/sec: 7567.02\n",
      "step 899, loss: 1.4455711841583252, dt: 66.30 ms, tokens/sec: 7722.59\n",
      "step 900, loss: 1.5817203521728516, dt: 65.80 ms, tokens/sec: 7781.64\n",
      "step 901, loss: 1.4357941150665283, dt: 65.64 ms, tokens/sec: 7799.79\n",
      "step 902, loss: 1.6138936281204224, dt: 66.32 ms, tokens/sec: 7720.40\n",
      "step 903, loss: 1.5130507946014404, dt: 61.77 ms, tokens/sec: 8288.82\n",
      "step 904, loss: 1.3954930305480957, dt: 57.07 ms, tokens/sec: 8971.74\n",
      "step 905, loss: 1.5130963325500488, dt: 56.74 ms, tokens/sec: 9024.25\n",
      "step 906, loss: 1.4466941356658936, dt: 56.64 ms, tokens/sec: 9038.76\n",
      "step 907, loss: 1.3237394094467163, dt: 56.76 ms, tokens/sec: 9020.58\n",
      "step 908, loss: 1.3762938976287842, dt: 56.38 ms, tokens/sec: 9081.08\n",
      "step 909, loss: 1.3368854522705078, dt: 56.67 ms, tokens/sec: 9035.11\n",
      "step 910, loss: 1.3023520708084106, dt: 57.11 ms, tokens/sec: 8964.51\n",
      "step 911, loss: 1.2792670726776123, dt: 57.82 ms, tokens/sec: 8855.35\n",
      "step 912, loss: 1.3713067770004272, dt: 58.22 ms, tokens/sec: 8794.71\n",
      "step 913, loss: 1.5973148345947266, dt: 58.14 ms, tokens/sec: 8805.89\n",
      "step 914, loss: 1.4576468467712402, dt: 59.05 ms, tokens/sec: 8670.71\n",
      "step 915, loss: 1.5294442176818848, dt: 59.68 ms, tokens/sec: 8578.54\n",
      "step 916, loss: 1.3748916387557983, dt: 59.41 ms, tokens/sec: 8618.20\n",
      "step 917, loss: 1.4178224802017212, dt: 57.52 ms, tokens/sec: 8900.97\n",
      "step 918, loss: 1.3904736042022705, dt: 57.77 ms, tokens/sec: 8863.20\n",
      "step 919, loss: 1.447727918624878, dt: 57.23 ms, tokens/sec: 8946.84\n",
      "step 920, loss: 1.4472262859344482, dt: 58.16 ms, tokens/sec: 8803.44\n",
      "step 921, loss: 1.3709983825683594, dt: 57.93 ms, tokens/sec: 8838.07\n",
      "step 922, loss: 1.3161131143569946, dt: 59.83 ms, tokens/sec: 8558.13\n",
      "step 923, loss: 1.3036167621612549, dt: 59.04 ms, tokens/sec: 8671.62\n",
      "step 924, loss: 1.250173807144165, dt: 58.65 ms, tokens/sec: 8729.18\n",
      "step 925, loss: 1.3015285730361938, dt: 59.17 ms, tokens/sec: 8653.35\n",
      "step 926, loss: 1.3194477558135986, dt: 58.60 ms, tokens/sec: 8737.49\n",
      "step 927, loss: 1.298597812652588, dt: 58.04 ms, tokens/sec: 8821.81\n",
      "step 928, loss: 1.2953240871429443, dt: 58.53 ms, tokens/sec: 8747.53\n",
      "step 929, loss: 1.3028167486190796, dt: 59.09 ms, tokens/sec: 8664.48\n",
      "step 930, loss: 1.2859039306640625, dt: 58.62 ms, tokens/sec: 8734.22\n",
      "step 931, loss: 1.184995174407959, dt: 57.80 ms, tokens/sec: 8858.42\n",
      "step 932, loss: 1.1534466743469238, dt: 59.86 ms, tokens/sec: 8553.67\n",
      "step 933, loss: 1.331268310546875, dt: 58.79 ms, tokens/sec: 8709.11\n",
      "step 934, loss: 1.2071499824523926, dt: 58.35 ms, tokens/sec: 8774.77\n",
      "step 935, loss: 1.2331864833831787, dt: 59.19 ms, tokens/sec: 8649.69\n",
      "step 936, loss: 1.2133718729019165, dt: 58.74 ms, tokens/sec: 8717.10\n",
      "step 937, loss: 1.237828016281128, dt: 58.77 ms, tokens/sec: 8711.80\n",
      "step 938, loss: 1.1117831468582153, dt: 58.29 ms, tokens/sec: 8783.24\n",
      "step 939, loss: 1.327131748199463, dt: 58.09 ms, tokens/sec: 8813.34\n",
      "step 940, loss: 1.216799259185791, dt: 58.53 ms, tokens/sec: 8747.96\n",
      "step 941, loss: 1.2081717252731323, dt: 58.39 ms, tokens/sec: 8768.03\n",
      "step 942, loss: 1.1471083164215088, dt: 57.58 ms, tokens/sec: 8891.39\n",
      "step 943, loss: 1.0574253797531128, dt: 61.01 ms, tokens/sec: 8391.92\n",
      "step 944, loss: 1.1421434879302979, dt: 65.91 ms, tokens/sec: 7767.59\n",
      "step 945, loss: 1.1263210773468018, dt: 65.11 ms, tokens/sec: 7863.88\n",
      "step 946, loss: 1.0812938213348389, dt: 59.24 ms, tokens/sec: 8642.20\n",
      "step 947, loss: 1.1127207279205322, dt: 58.25 ms, tokens/sec: 8789.56\n",
      "step 948, loss: 1.0678048133850098, dt: 56.72 ms, tokens/sec: 9027.44\n",
      "step 949, loss: 1.1309574842453003, dt: 62.17 ms, tokens/sec: 8235.64\n",
      "step 950, loss: 1.0104835033416748, dt: 66.69 ms, tokens/sec: 7676.82\n",
      "step 951, loss: 1.0777473449707031, dt: 72.06 ms, tokens/sec: 7104.99\n",
      "step 952, loss: 1.0594279766082764, dt: 73.09 ms, tokens/sec: 7004.60\n",
      "step 953, loss: 1.073930263519287, dt: 71.74 ms, tokens/sec: 7137.08\n",
      "step 954, loss: 0.9553152322769165, dt: 68.99 ms, tokens/sec: 7421.26\n",
      "step 955, loss: 1.1568470001220703, dt: 68.54 ms, tokens/sec: 7470.42\n",
      "step 956, loss: 1.1205079555511475, dt: 69.19 ms, tokens/sec: 7400.01\n",
      "step 957, loss: 0.9540095925331116, dt: 67.42 ms, tokens/sec: 7593.97\n",
      "step 958, loss: 0.9396485090255737, dt: 64.41 ms, tokens/sec: 7948.46\n",
      "step 959, loss: 0.8497334718704224, dt: 60.17 ms, tokens/sec: 8509.67\n",
      "step 960, loss: 0.8614756464958191, dt: 55.83 ms, tokens/sec: 9170.54\n",
      "step 961, loss: 0.8549842834472656, dt: 55.42 ms, tokens/sec: 9239.23\n",
      "step 962, loss: 0.9235299825668335, dt: 55.32 ms, tokens/sec: 9255.96\n",
      "step 963, loss: 0.907390832901001, dt: 59.34 ms, tokens/sec: 8628.52\n",
      "step 964, loss: 0.9423363208770752, dt: 58.09 ms, tokens/sec: 8813.44\n",
      "step 965, loss: 0.8819423913955688, dt: 56.97 ms, tokens/sec: 8987.84\n",
      "step 966, loss: 0.8310574293136597, dt: 57.51 ms, tokens/sec: 8902.37\n",
      "step 967, loss: 1.0333170890808105, dt: 58.66 ms, tokens/sec: 8729.01\n",
      "step 968, loss: 0.9659003019332886, dt: 58.28 ms, tokens/sec: 8785.03\n",
      "step 969, loss: 0.950913667678833, dt: 59.80 ms, tokens/sec: 8561.41\n",
      "step 970, loss: 0.9185014963150024, dt: 59.08 ms, tokens/sec: 8665.50\n",
      "step 971, loss: 0.9957082271575928, dt: 59.65 ms, tokens/sec: 8582.79\n",
      "step 972, loss: 1.0933837890625, dt: 59.01 ms, tokens/sec: 8676.21\n",
      "step 973, loss: 0.8505688905715942, dt: 58.78 ms, tokens/sec: 8710.74\n",
      "step 974, loss: 0.9053303599357605, dt: 59.92 ms, tokens/sec: 8544.31\n",
      "step 975, loss: 1.036614179611206, dt: 59.10 ms, tokens/sec: 8663.02\n",
      "step 976, loss: 0.9053789973258972, dt: 59.05 ms, tokens/sec: 8670.92\n",
      "step 977, loss: 1.049713373184204, dt: 58.59 ms, tokens/sec: 8738.99\n",
      "step 978, loss: 1.0080313682556152, dt: 60.72 ms, tokens/sec: 8431.75\n",
      "step 979, loss: 0.8626642227172852, dt: 59.40 ms, tokens/sec: 8618.82\n",
      "step 980, loss: 0.9714913964271545, dt: 58.71 ms, tokens/sec: 8721.10\n",
      "step 981, loss: 0.901881217956543, dt: 58.83 ms, tokens/sec: 8703.78\n",
      "step 982, loss: 0.8279415965080261, dt: 57.72 ms, tokens/sec: 8869.98\n",
      "step 983, loss: 0.9720048904418945, dt: 58.66 ms, tokens/sec: 8727.62\n",
      "step 984, loss: 0.8673908710479736, dt: 57.78 ms, tokens/sec: 8860.90\n",
      "step 985, loss: 0.777172327041626, dt: 57.66 ms, tokens/sec: 8879.37\n",
      "step 986, loss: 0.9947385191917419, dt: 59.11 ms, tokens/sec: 8661.83\n",
      "step 987, loss: 0.8174614906311035, dt: 58.92 ms, tokens/sec: 8689.31\n",
      "step 988, loss: 0.9459002017974854, dt: 57.95 ms, tokens/sec: 8834.73\n",
      "step 989, loss: 0.9103859663009644, dt: 59.26 ms, tokens/sec: 8639.18\n",
      "step 990, loss: 0.9813569784164429, dt: 58.86 ms, tokens/sec: 8698.32\n",
      "step 991, loss: 0.7842023968696594, dt: 61.73 ms, tokens/sec: 8293.65\n",
      "step 992, loss: 0.9850252866744995, dt: 64.09 ms, tokens/sec: 7988.50\n",
      "step 993, loss: 0.8726100325584412, dt: 66.88 ms, tokens/sec: 7655.18\n",
      "step 994, loss: 0.9156858921051025, dt: 69.58 ms, tokens/sec: 7358.02\n",
      "step 995, loss: 0.8920596837997437, dt: 69.73 ms, tokens/sec: 7342.80\n",
      "step 996, loss: 0.8501847386360168, dt: 67.94 ms, tokens/sec: 7536.27\n",
      "step 997, loss: 0.8817375898361206, dt: 66.79 ms, tokens/sec: 7666.27\n",
      "step 998, loss: 0.9168342351913452, dt: 66.10 ms, tokens/sec: 7745.27\n",
      "step 999, loss: 0.9101873636245728, dt: 67.56 ms, tokens/sec: 7578.43\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "import time \n",
    "train_loader = DataLoaderLite(B=8, T=64)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_rt_seqs = 1\n",
    "max_len  = 100\n",
    "\n",
    "model = GPT(GPTConfig)\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas = (0.9, 0.95), eps= 10e-8)\n",
    "for i in range(1000):\n",
    "    t0=time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y= x.to(device), y.to(device)\n",
    "    with torch.autocast(device_type=device, dtype = torch.bfloat16): # 16 bit floating point\n",
    "        logits,loss = model(x,y)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    dt = time.time() - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    print(f\"step {i}, loss: {loss.item()}, dt: {dt*1000:.2f} ms, tokens/sec: {tokens_per_sec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe9441e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> FAUSTLook, who\n",
      "\n",
      "My friend, not know everything, see her?\n",
      "\n",
      "A face, whos there, I was so all this pleasure, all all his hand,\n",
      "And who must trouble I find-d give?\n",
      "\n",
      "For while man is the worst of grieve yous de who never seen the room, on you are, I was more, on.\n",
      "\n",
      "I do dislike canm right well.\t\t\t\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "#tokens = enc.encode(\"You are helpful assistant. What is 2+2?\")\n",
    "tokens = enc.encode(\"FAUST\")\n",
    "tokens = torch.tensor(tokens, dtype = torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_rt_seqs, 1)\n",
    "x = tokens.to(device)\n",
    "\n",
    "while x.size(1) < max_len:\n",
    "    with torch.no_grad():\n",
    "        logits,loss = model(x)\n",
    "        logits = logits[:,-1,:]\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        top_probs, top_ind = torch.topk(probs, 50, dim = -1)\n",
    "        ix = torch.multinomial(top_probs,1)\n",
    "        xcol = torch.gather(top_ind, -1, ix)\n",
    "        x = torch.cat((x,xcol), dim = 1)\n",
    "#\n",
    "for i in range(num_rt_seqs):\n",
    "    tokens = x[i, :max_len].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
